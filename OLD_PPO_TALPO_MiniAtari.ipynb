{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-07T17:48:38.529646Z",
     "iopub.status.busy": "2024-12-07T17:48:38.529255Z",
     "iopub.status.idle": "2024-12-07T17:50:02.514573Z",
     "shell.execute_reply": "2024-12-07T17:50:02.513748Z",
     "shell.execute_reply.started": "2024-12-07T17:48:38.529595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install minatar\n",
    "!pip install dm-haiku\n",
    "!pip install distrax\n",
    "!pip install pgx\n",
    "!pip install omegaconf\n",
    "!pip install learn2learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:50:02.517574Z",
     "iopub.status.busy": "2024-12-07T17:50:02.516806Z",
     "iopub.status.idle": "2024-12-07T17:50:24.307644Z",
     "shell.execute_reply": "2024-12-07T17:50:24.306931Z",
     "shell.execute_reply.started": "2024-12-07T17:50:02.517533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "from typing import NamedTuple, Literal\n",
    "import distrax\n",
    "import pgx\n",
    "from pgx.experimental import auto_reset\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from omegaconf import OmegaConf\n",
    "from pydantic import BaseModel\n",
    "import wandb\n",
    "import learn2learn as l2l\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T13:11:18.944825Z",
     "iopub.status.busy": "2024-12-06T13:11:18.944542Z",
     "iopub.status.idle": "2024-12-06T13:11:29.076455Z",
     "shell.execute_reply": "2024-12-06T13:11:29.074694Z",
     "shell.execute_reply.started": "2024-12-06T13:11:18.944799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration class remains unchanged\n",
    "class PPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-space_invaders\" \n",
    "    seed: int = 0\n",
    "    lr: float = 1e-3\n",
    "    num_envs: int = 64 \n",
    "    num_eval_envs: int = 100\n",
    "    num_steps: int = 128\n",
    "    total_timesteps: int = int(1e7)\n",
    "    update_epochs: int = 4\n",
    "    minibatch_size: int = 4096\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    wandb_entity: str = \"alfiovavassori-universit-della-svizzera-italiana-org\" \n",
    "    wandb_project: str = \"pgx-minatar-ppo\"  \n",
    "    save_model: bool = False\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "args = PPOConfig()\n",
    "\n",
    "# Initialize the environment\n",
    "env = pgx.make(str(args.env_name))\n",
    "\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n",
    "\n",
    "# Actor-Critic model definition\n",
    "class ActorCritic(hk.Module):\n",
    "    def __init__(self, num_actions, activation=\"tanh\"):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.activation = activation\n",
    "        assert activation in [\"relu\", \"tanh\"]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32)\n",
    "        if self.activation == \"relu\":\n",
    "            activation = jax.nn.relu\n",
    "        else:\n",
    "            activation = jax.nn.tanh\n",
    "        x = hk.Conv2D(32, kernel_shape=2)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = hk.avg_pool(x, window_shape=(2, 2),\n",
    "                        strides=(2, 2), padding=\"VALID\")\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = hk.Linear(64)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        actor_mean = hk.Linear(64)(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = hk.Linear(64)(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = hk.Linear(self.num_actions)(actor_mean)\n",
    "\n",
    "        critic = hk.Linear(64)(x)\n",
    "        critic = activation(critic)\n",
    "        critic = hk.Linear(64)(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = hk.Linear(1)(critic)\n",
    "\n",
    "        return actor_mean, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "def forward_fn(x, is_eval=False):\n",
    "    net = ActorCritic(env.num_actions, activation=\"tanh\")\n",
    "    logits, value = net(x)\n",
    "    return logits, value\n",
    "\n",
    "\n",
    "forward = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "\n",
    "\n",
    "optimizer = optax.chain(optax.clip_by_global_norm(\n",
    "    args.max_grad_norm), optax.adam(args.lr, eps=1e-5))\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_update_fn():\n",
    "    # TRAIN LOOP\n",
    "    def _update_step(runner_state):\n",
    "        # COLLECT TRAJECTORIES\n",
    "        step_fn = jax.vmap(auto_reset(env.step, env.init))\n",
    "\n",
    "        def _env_step(runner_state, unused):\n",
    "            params, opt_state, env_state, last_obs, rng = runner_state\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = forward.apply(params, last_obs)\n",
    "            pi = distrax.Categorical(logits=logits)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            keys = jax.random.split(_rng, env_state.observation.shape[0])\n",
    "            env_state = step_fn(env_state, action, keys)\n",
    "            transition = Transition(\n",
    "                env_state.terminated,\n",
    "                action,\n",
    "                value,\n",
    "                jnp.squeeze(env_state.rewards),\n",
    "                log_prob,\n",
    "                last_obs\n",
    "            )\n",
    "            runner_state = (params, opt_state, env_state,\n",
    "                            env_state.observation, rng)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, runner_state, None, args.num_steps\n",
    "        )\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "        params, opt_state, env_state, last_obs, rng = runner_state\n",
    "        _, last_val = forward.apply(params, last_obs)\n",
    "\n",
    "        def _calculate_gae(traj_batch, last_val):\n",
    "            def _get_advantages(gae_and_next_value, transition):\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                )\n",
    "                delta = reward + args.gamma * next_value * (1 - done) - value\n",
    "                gae = (\n",
    "                    delta\n",
    "                    + args.gamma * args.gae_lambda * (1 - done) * gae\n",
    "                )\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return advantages, advantages + traj_batch.value\n",
    "\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        # UPDATE NETWORK\n",
    "        def _update_epoch(update_state, unused):\n",
    "            def _update_minbatch(tup, batch_info):\n",
    "                params, opt_state = tup\n",
    "                traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                def _loss_fn(params, traj_batch, gae, targets):\n",
    "                    # RERUN NETWORK\n",
    "                    logits, value = forward.apply(params, traj_batch.obs)\n",
    "                    pi = distrax.Categorical(logits=logits)\n",
    "                    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                    # CALCULATE VALUE LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (\n",
    "                        value - traj_batch.value\n",
    "                    ).clip(-args.clip_eps, args.clip_eps)\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(\n",
    "                        value_pred_clipped - targets)\n",
    "                    value_loss = (\n",
    "                        0.5 * jnp.maximum(value_losses,\n",
    "                                          value_losses_clipped).mean()\n",
    "                    )\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = (\n",
    "                        jnp.clip(\n",
    "                            ratio,\n",
    "                            1.0 - args.clip_eps,\n",
    "                            1.0 + args.clip_eps,\n",
    "                        )\n",
    "                        * gae\n",
    "                    )\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                    loss_actor = loss_actor.mean()\n",
    "                    entropy = pi.entropy().mean()\n",
    "\n",
    "                    total_loss = (\n",
    "                        loss_actor\n",
    "                        + args.vf_coef * value_loss\n",
    "                        - args.ent_coef * entropy\n",
    "                    )\n",
    "                    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                total_loss, grads = grad_fn(\n",
    "                    params, traj_batch, advantages, targets)\n",
    "                updates, opt_state = optimizer.update(grads, opt_state)\n",
    "                params = optax.apply_updates(params, updates)\n",
    "                return (params, opt_state), total_loss\n",
    "\n",
    "            params, opt_state, traj_batch, advantages, targets, rng = update_state\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            batch_size = args.minibatch_size * num_minibatches\n",
    "            assert (\n",
    "                batch_size == args.num_steps * args.num_envs\n",
    "            ), \"batch size must be equal to number of steps * number of envs\"\n",
    "            permutation = jax.random.permutation(_rng, batch_size)\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree_util.tree_map(\n",
    "                lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "            )\n",
    "            shuffled_batch = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "            )\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(\n",
    "                    x, [num_minibatches, -1] + list(x.shape[1:])\n",
    "                ),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (params, opt_state),  total_loss = jax.lax.scan(\n",
    "                _update_minbatch, (params, opt_state), minibatches\n",
    "            )\n",
    "            update_state = (params, opt_state, traj_batch,\n",
    "                            advantages, targets, rng)\n",
    "            return update_state, total_loss\n",
    "\n",
    "        update_state = (params, opt_state, traj_batch,\n",
    "                        advantages, targets, rng)\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, args.update_epochs\n",
    "        )\n",
    "        params, opt_state, _, _, _, rng = update_state\n",
    "\n",
    "        runner_state = (params, opt_state, env_state, last_obs, rng)\n",
    "        return runner_state, loss_info\n",
    "    return _update_step\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, rng_key):\n",
    "    step_fn = jax.vmap(env.step)\n",
    "    rng_key, sub_key = jax.random.split(rng_key)\n",
    "    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n",
    "    state = jax.vmap(env.init)(subkeys)\n",
    "    R = jnp.zeros_like(state.rewards)\n",
    "\n",
    "    def cond_fn(tup):\n",
    "        state, _, _ = tup\n",
    "        return ~state.terminated.all()\n",
    "\n",
    "    def loop_fn(tup):\n",
    "        state, R, rng_key = tup\n",
    "        logits, value = forward.apply(params, state.observation)\n",
    "        # action = logits.argmax(axis=-1)\n",
    "        pi = distrax.Categorical(logits=logits)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        action = pi.sample(seed=_rng)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        keys = jax.random.split(_rng, state.observation.shape[0])\n",
    "        state = step_fn(state, action, keys)\n",
    "        return state, R + state.rewards, rng_key\n",
    "    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n",
    "    return R.mean()\n",
    "\n",
    "\n",
    "def train(rng):\n",
    "    tt = 0\n",
    "    st = time.time()\n",
    "    # INIT NETWORK\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    init_x = jnp.zeros((1, ) + env.observation_shape)\n",
    "    params = forward.init(_rng, init_x)\n",
    "    opt_state = optimizer.init(params=params)\n",
    "\n",
    "    # INIT UPDATE FUNCTION\n",
    "    _update_step = make_update_fn()\n",
    "    jitted_update_step = jax.jit(_update_step)\n",
    "\n",
    "    # INIT ENV\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, args.num_envs)\n",
    "    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (params, opt_state, env_state, env_state.observation, _rng)\n",
    "\n",
    "    # warm up\n",
    "    _, _ = jitted_update_step(runner_state)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    # initial evaluation\n",
    "    et = time.time()  # exclude evaluation time\n",
    "    tt += et - st\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    eval_R = evaluate(runner_state[0], _rng)\n",
    "    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "    print(log)\n",
    "    wandb.log(log)\n",
    "    st = time.time()\n",
    "\n",
    "    for i in range(num_updates):\n",
    "        runner_state, loss_info = jitted_update_step(runner_state)\n",
    "        steps += args.num_envs * args.num_steps\n",
    "\n",
    "        # evaluation\n",
    "        et = time.time()  # exclude evaluation time\n",
    "        tt += et - st\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        eval_R = evaluate(runner_state[0], _rng)\n",
    "        log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "        print(log)\n",
    "        wandb.log(log)\n",
    "        st = time.time()\n",
    "\n",
    "    return runner_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args.dict())\n",
    "    rng = jax.random.PRNGKey(args.seed)\n",
    "    out = train(rng)\n",
    "    if args.save_model:\n",
    "        with open(f\"{args.env_name}-seed={args.seed}.ckpt\", \"wb\") as f:\n",
    "            pickle.dump(out[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:11:52.473337Z",
     "iopub.status.busy": "2024-12-07T18:11:52.473018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration class for DPO\n",
    "class DPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-space_invaders\"\n",
    "    seed: int = 2\n",
    "    lr: float = 5e-4\n",
    "    num_envs: int = 2048\n",
    "    num_eval_envs: int = 128\n",
    "    num_steps: int = 5\n",
    "    total_timesteps: int = 10000000\n",
    "    update_epochs: int = 4\n",
    "    minibatch_size: int = 1024\n",
    "    num_minibatches: int = 32\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 8.0\n",
    "    dpo_alpha: float = 8.0\n",
    "    dpo_beta: float = 0.6\n",
    "    wandb_entity: str = \"your-wandb-entity\"\n",
    "    wandb_project: str = \"pgx-minatar-dpo\"\n",
    "    save_model: bool = False\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "args = DPOConfig()\n",
    "\n",
    "# Initialize the environment\n",
    "env = pgx.make(str(args.env_name))\n",
    "\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n",
    "\n",
    "# Actor-Critic model definition\n",
    "class ActorCritic(hk.Module):\n",
    "    def __init__(self, num_actions, activation=\"tanh\"):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32)\n",
    "        activation = jax.nn.tanh if self.activation == \"tanh\" else jax.nn.relu\n",
    "        x = hk.Conv2D(32, kernel_shape=2)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = hk.avg_pool(x, window_shape=(2, 2), strides=(2, 2), padding=\"VALID\")\n",
    "        x = x.reshape((x.shape[0], -1))  # Flatten\n",
    "        x = hk.Linear(64)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        actor_mean = hk.Linear(self.num_actions)(activation(hk.Linear(64)(activation(hk.Linear(64)(x)))))\n",
    "        critic = hk.Linear(1)(activation(hk.Linear(64)(activation(hk.Linear(64)(x)))))\n",
    "        return actor_mean, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "def forward_fn(x):\n",
    "    net = ActorCritic(env.num_actions, activation=\"tanh\")\n",
    "    logits, value = net(x)\n",
    "    return logits, value\n",
    "\n",
    "forward = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(args.max_grad_norm),\n",
    "    optax.adam(args.lr, eps=1e-5)\n",
    ")\n",
    "\n",
    "# Transition class\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    lifetime_info: jnp.ndarray  # Temporal information for TA-LPO\n",
    "\n",
    "# Generalized Advantage Estimation\n",
    "def _calculate_gae(traj_batch, last_val):\n",
    "    def _get_advantages(gae_and_next_value, transition):\n",
    "        gae, next_value = gae_and_next_value\n",
    "        delta = transition.reward + args.gamma * next_value * (1 - transition.done) - transition.value\n",
    "        gae = delta + args.gamma * args.gae_lambda * (1 - transition.done) * gae\n",
    "        return (gae, transition.value), gae\n",
    "\n",
    "    _, advantages = jax.lax.scan(\n",
    "        _get_advantages,\n",
    "        (jnp.zeros_like(last_val), last_val),\n",
    "        traj_batch,\n",
    "        reverse=True\n",
    "    )\n",
    "    return advantages, advantages + traj_batch.value\n",
    "\n",
    "# Drift-based Loss Function\n",
    "def _loss_fn(params, traj_batch, advantages, targets, lifetime_info):\n",
    "    logits, value = forward.apply(params, traj_batch.obs)\n",
    "    pi = distrax.Categorical(logits=logits)\n",
    "    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "    # Critic Loss\n",
    "    value_pred_clipped = traj_batch.value + (\n",
    "        value - traj_batch.value\n",
    "    ).clip(-args.clip_eps, args.clip_eps)\n",
    "    value_losses = jnp.square(value - targets)\n",
    "    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "    # Drift Function Logic\n",
    "    alpha = args.dpo_alpha\n",
    "    beta = args.dpo_beta\n",
    "    log_diff = log_prob - traj_batch.log_prob\n",
    "    ratio = jnp.exp(log_diff)\n",
    "    gae = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # Temporal Factor for TA-LPO\n",
    "    temporal_factor = lifetime_info[..., None]\n",
    "    is_pos = (gae >= 0.0).astype(\"float32\")\n",
    "    drift1 = nn.relu(ratio - 1.0 - alpha * nn.tanh((ratio - 1.0) / alpha)) * temporal_factor\n",
    "    drift2 = nn.relu(log_diff - beta * nn.tanh(log_diff / beta)) * (1 - temporal_factor)\n",
    "    drift = drift1 * is_pos + drift2 * (1 - is_pos)\n",
    "    loss_actor = -(ratio * gae - drift).mean()\n",
    "\n",
    "    # Entropy Regularization\n",
    "    entropy = pi.entropy().mean()\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_actor + args.vf_coef * value_loss - args.ent_coef * entropy\n",
    "    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "def make_update_fn():\n",
    "    def _update_step(runner_state):\n",
    "        step_fn = jax.vmap(env.step)\n",
    "\n",
    "        def _env_step(runner_state, unused):\n",
    "            params, opt_state, env_state, last_obs, rng, step_count = runner_state\n",
    "\n",
    "            # Calculate Lifetime Information (Temporal Awareness)\n",
    "            total_steps = args.num_steps * args.num_envs\n",
    "            lifetime_info = jnp.array([step_count / total_steps], dtype=jnp.float32)\n",
    "\n",
    "            # Select Action\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = forward.apply(params, last_obs)\n",
    "            pi = distrax.Categorical(logits=logits)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # Step Environment\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            keys = jax.random.split(_rng, env_state.observation.shape[0])\n",
    "            env_state = step_fn(env_state, action, keys)\n",
    "\n",
    "            # Create Transition\n",
    "            transition = Transition(\n",
    "                env_state.terminated,\n",
    "                action,\n",
    "                value,\n",
    "                jnp.squeeze(env_state.rewards),\n",
    "                log_prob,\n",
    "                last_obs,\n",
    "                lifetime_info\n",
    "            )\n",
    "\n",
    "            # Update Runner State\n",
    "            runner_state = (params, opt_state, env_state, env_state.observation, rng, step_count + 1)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, args.num_steps)\n",
    "        params, opt_state, env_state, last_obs, rng, step_count = runner_state\n",
    "        _, last_val = forward.apply(params, last_obs)\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        def _update_epoch(update_state, unused):\n",
    "            def _update_minbatch(tup, batch_info):\n",
    "                params, opt_state = tup\n",
    "                traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                def _loss_fn(params, traj_batch, gae, targets):\n",
    "                    logits, value = forward.apply(params, traj_batch.obs)\n",
    "                    pi = distrax.Categorical(logits=logits)\n",
    "                    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                    # Critic Loss\n",
    "                    value_pred_clipped = traj_batch.value + (\n",
    "                        value - traj_batch.value\n",
    "                    ).clip(-args.clip_eps, args.clip_eps)\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "                    # Actor Loss with DPO\n",
    "                    alpha = args.dpo_alpha\n",
    "                    beta = args.dpo_beta\n",
    "                    log_diff = log_prob - traj_batch.log_prob\n",
    "                    ratio = jnp.exp(log_diff)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    is_pos = (gae >= 0.0).astype(\"float32\")\n",
    "                    drift1 = nn.relu(ratio - 1.0 - alpha * nn.tanh((ratio - 1.0) / alpha))\n",
    "                    drift2 = nn.relu(log_diff - beta * nn.tanh(log_diff / beta))\n",
    "                    drift = drift1 * is_pos + drift2 * (1 - is_pos)\n",
    "                    loss_actor = -(ratio * gae - drift).mean()\n",
    "\n",
    "                    # Entropy Regularization\n",
    "                    entropy = pi.entropy().mean()\n",
    "\n",
    "                    # Total Loss\n",
    "                    total_loss = (\n",
    "                        loss_actor\n",
    "                        + args.vf_coef * value_loss\n",
    "                        - args.ent_coef * entropy\n",
    "                    )\n",
    "                    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                total_loss, grads = grad_fn(params, traj_batch, advantages, targets)\n",
    "                updates, opt_state = optimizer.update(grads, opt_state)\n",
    "                params = optax.apply_updates(params, updates)\n",
    "                return (params, opt_state), total_loss\n",
    "\n",
    "            params, opt_state, traj_batch, advantages, targets, rng = update_state\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            batch_size = args.minibatch_size * num_minibatches\n",
    "            assert batch_size == args.num_steps * args.num_envs, \"Batch size mismatch.\"\n",
    "            permutation = jax.random.permutation(_rng, batch_size)\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree_util.tree_map(lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
    "            shuffled_batch = jax.tree_util.tree_map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(x, [num_minibatches, -1] + list(x.shape[1:])),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (params, opt_state), _ = jax.lax.scan(_update_minbatch, (params, opt_state), minibatches)\n",
    "            update_state = (params, opt_state, traj_batch, advantages, targets, rng)\n",
    "            return update_state, None\n",
    "\n",
    "        update_state = (params, opt_state, traj_batch, advantages, targets, rng)\n",
    "        update_state, _ = jax.lax.scan(_update_epoch, update_state, None, args.update_epochs)\n",
    "        params, opt_state, _, _, _, rng = update_state\n",
    "        runner_state = (params, opt_state, env_state, last_obs, rng, step_count)\n",
    "        return runner_state, None\n",
    "\n",
    "    return _update_step\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, rng_key):\n",
    "    step_fn = jax.vmap(env.step)\n",
    "    rng_key, sub_key = jax.random.split(rng_key)\n",
    "    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n",
    "    state = jax.vmap(env.init)(subkeys)\n",
    "    R = jnp.zeros_like(state.rewards)\n",
    "\n",
    "    def cond_fn(tup):\n",
    "        state, _, _ = tup\n",
    "        return ~state.terminated.all()\n",
    "\n",
    "    def loop_fn(tup):\n",
    "        state, R, rng_key = tup\n",
    "        logits, _ = forward.apply(params, state.observation)\n",
    "        pi = distrax.Categorical(logits=logits)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        action = pi.sample(seed=_rng)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        keys = jax.random.split(_rng, state.observation.shape[0])\n",
    "        state = step_fn(state, action, keys)\n",
    "        return state, R + state.rewards, rng_key\n",
    "\n",
    "    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n",
    "    return R.mean()\n",
    "\n",
    "def train(rng):\n",
    "    tt = 0\n",
    "    st = time.time()\n",
    "\n",
    "    # Initialize Network\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    init_x = jnp.zeros((1,) + env.observation_shape)\n",
    "    params = forward.init(_rng, init_x)\n",
    "    opt_state = optimizer.init(params=params)\n",
    "\n",
    "    # Initialize Update Function\n",
    "    _update_step = make_update_fn()\n",
    "    jitted_update_step = jax.jit(_update_step)\n",
    "\n",
    "    # Initialize Environment\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, args.num_envs)\n",
    "    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (params, opt_state, env_state, env_state.observation, _rng)\n",
    "\n",
    "    # Warm-Up\n",
    "    _, _ = jitted_update_step(runner_state)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    # Initial Evaluation\n",
    "    et = time.time()  # exclude evaluation time\n",
    "    tt += et - st\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    eval_R = evaluate(runner_state[0], _rng)\n",
    "    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "    print(log)\n",
    "    wandb.log(log)\n",
    "    st = time.time()\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(num_updates):\n",
    "        runner_state, loss_info = jitted_update_step(runner_state)\n",
    "        steps += args.num_envs * args.num_steps\n",
    "\n",
    "        # Evaluation\n",
    "        et = time.time()  # exclude evaluation time\n",
    "        tt += et - st\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        eval_R = evaluate(runner_state[0], _rng)\n",
    "        log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "        print(log)\n",
    "        wandb.log(log)\n",
    "        st = time.time()\n",
    "\n",
    "    return runner_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args.dict())\n",
    "    rng = jax.random.PRNGKey(args.seed)\n",
    "    out = train(rng)\n",
    "    if args.save_model:\n",
    "        with open(f\"{args.env_name}-seed={args.seed}.ckpt\", \"wb\") as f:\n",
    "            pickle.dump(out[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for PPO without TA\n",
    "class PPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-space_invaders\"\n",
    "    seed: int = 0\n",
    "    lr: float = 1e-3\n",
    "    num_envs: int = 64\n",
    "    num_eval_envs: int = 100\n",
    "    num_steps: int = 128\n",
    "    total_timesteps: int = int(1e7)\n",
    "    update_epochs: int = 4\n",
    "    minibatch_size: int = 4096\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    wandb_entity: str = \"your-wandb-entity\"\n",
    "    wandb_project: str = \"pgx-minatar-ppo\"\n",
    "    save_model: bool = False\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "args = PPOConfig()\n",
    "\n",
    "# Initialize the environment\n",
    "env = pgx.make(str(args.env_name))\n",
    "\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n",
    "\n",
    "# Actor-Critic model definition\n",
    "class ActorCritic(hk.Module):\n",
    "    def __init__(self, num_actions, activation=\"tanh\"):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32)\n",
    "        activation = jax.nn.tanh if self.activation == \"tanh\" else jax.nn.relu\n",
    "        x = hk.Conv2D(32, kernel_shape=2)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = hk.avg_pool(x, window_shape=(2, 2), strides=(2, 2), padding=\"VALID\")\n",
    "        x = x.reshape((x.shape[0], -1))  # Flatten\n",
    "        x = hk.Linear(64)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        actor_mean = hk.Linear(self.num_actions)(activation(hk.Linear(64)(activation(hk.Linear(64)(x)))))\n",
    "        critic = hk.Linear(1)(activation(hk.Linear(64)(activation(hk.Linear(64)(x)))))\n",
    "        return actor_mean, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "def forward_fn(x):\n",
    "    net = ActorCritic(env.num_actions, activation=\"tanh\")\n",
    "    logits, value = net(x)\n",
    "    return logits, value\n",
    "\n",
    "forward = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(args.max_grad_norm),\n",
    "    optax.adam(args.lr, eps=1e-5)\n",
    ")\n",
    "\n",
    "# Transition class\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "\n",
    "# Generalized Advantage Estimation\n",
    "def _calculate_gae(traj_batch, last_val):\n",
    "    def _get_advantages(gae_and_next_value, transition):\n",
    "        gae, next_value = gae_and_next_value\n",
    "        delta = transition.reward + args.gamma * next_value * (1 - transition.done) - transition.value\n",
    "        gae = delta + args.gamma * args.gae_lambda * (1 - transition.done) * gae\n",
    "        return (gae, transition.value), gae\n",
    "\n",
    "    _, advantages = jax.lax.scan(\n",
    "        _get_advantages,\n",
    "        (jnp.zeros_like(last_val), last_val),\n",
    "        traj_batch,\n",
    "        reverse=True\n",
    "    )\n",
    "    return advantages, advantages + traj_batch.value\n",
    "\n",
    "def _loss_fn(params, traj_batch, advantages, targets):\n",
    "    logits, value = forward.apply(params, traj_batch.obs)\n",
    "    pi = distrax.Categorical(logits=logits)\n",
    "    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "    # Critic Loss\n",
    "    value_pred_clipped = traj_batch.value + (\n",
    "        value - traj_batch.value\n",
    "    ).clip(-args.clip_eps, args.clip_eps)\n",
    "    value_losses = jnp.square(value - targets)\n",
    "    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "    # Actor Loss\n",
    "    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "    gae = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    loss_actor1 = ratio * gae\n",
    "    loss_actor2 = jnp.clip(ratio, 1.0 - args.clip_eps, 1.0 + args.clip_eps) * gae\n",
    "    loss_actor = -jnp.minimum(loss_actor1, loss_actor2).mean()\n",
    "\n",
    "    # Entropy Regularization\n",
    "    entropy = pi.entropy().mean()\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_actor + args.vf_coef * value_loss - args.ent_coef * entropy\n",
    "    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "def make_update_fn():\n",
    "    def _update_step(runner_state):\n",
    "        step_fn = jax.vmap(env.step)\n",
    "\n",
    "        def _env_step(runner_state, unused):\n",
    "            params, opt_state, env_state, last_obs, rng = runner_state\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = forward.apply(params, last_obs)\n",
    "            pi = distrax.Categorical(logits=logits)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            keys = jax.random.split(_rng, env_state.observation.shape[0])\n",
    "            env_state = step_fn(env_state, action, keys)\n",
    "            transition = Transition(\n",
    "                env_state.terminated,\n",
    "                action,\n",
    "                value,\n",
    "                jnp.squeeze(env_state.rewards),\n",
    "                log_prob,\n",
    "                last_obs,\n",
    "            )\n",
    "            runner_state = (params, opt_state, env_state, env_state.observation, rng)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, args.num_steps)\n",
    "        params, opt_state, env_state, last_obs, rng = runner_state\n",
    "        _, last_val = forward.apply(params, last_obs)\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        def _update_epoch(update_state, unused):\n",
    "            def _update_minbatch(tup, batch_info):\n",
    "                params, opt_state = tup\n",
    "                traj_batch, advantages, targets = batch_info\n",
    "                grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                total_loss, grads = grad_fn(params, traj_batch, advantages, targets)\n",
    "                updates, opt_state = optimizer.update(grads, opt_state)\n",
    "                params = optax.apply_updates(params, updates)\n",
    "                return (params, opt_state), total_loss\n",
    "\n",
    "            params, opt_state, traj_batch, advantages, targets, rng = update_state\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            batch_size = args.minibatch_size * num_minibatches\n",
    "            assert batch_size == args.num_steps * args.num_envs, \"Batch size mismatch.\"\n",
    "            permutation = jax.random.permutation(_rng, batch_size)\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree_util.tree_map(lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
    "            shuffled_batch = jax.tree_util.tree_map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(x, [num_minibatches, -1] + list(x.shape[1:])),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (params, opt_state), _ = jax.lax.scan(_update_minbatch, (params, opt_state), minibatches)\n",
    "            update_state = (params, opt_state, traj_batch, advantages, targets, rng)\n",
    "            return update_state, None\n",
    "\n",
    "        update_state = (params, opt_state, traj_batch, advantages, targets, rng)\n",
    "        update_state, _ = jax.lax.scan(_update_epoch, update_state, None, args.update_epochs)\n",
    "        params, opt_state, _, _, _ = update_state\n",
    "        runner_state = (params, opt_state, env_state, last_obs, rng)\n",
    "        return runner_state, None\n",
    "\n",
    "    return _update_step\n",
    "\n",
    "# Evaluation Function\n",
    "@jax.jit\n",
    "def evaluate(params, rng_key):\n",
    "    step_fn = jax.vmap(env.step)\n",
    "    rng_key, sub_key = jax.random.split(rng_key)\n",
    "    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n",
    "    state = jax.vmap(env.init)(subkeys)\n",
    "    R = jnp.zeros_like(state.rewards)\n",
    "\n",
    "    def cond_fn(tup):\n",
    "        state, _, _ = tup\n",
    "        return ~state.terminated.all()\n",
    "\n",
    "    def loop_fn(tup):\n",
    "        state, R, rng_key = tup\n",
    "        logits, _ = forward.apply(params, state.observation)\n",
    "        pi = distrax.Categorical(logits=logits)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        action = pi.sample(seed=_rng)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        keys = jax.random.split(_rng, state.observation.shape[0])\n",
    "        state = step_fn(state, action, keys)\n",
    "        return state, R + state.rewards, rng_key\n",
    "\n",
    "    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n",
    "    return R.mean()\n",
    "\n",
    "# Training Function\n",
    "def train(rng, mode=\"ppo\"):\n",
    "    \"\"\"\n",
    "    Train function to switch between PPO, DPO, and LPO.\n",
    "    `mode`: Can be \"ppo\", \"dpo\", or \"lpo\".\n",
    "    \"\"\"\n",
    "    tt = 0\n",
    "    st = time.time()\n",
    "\n",
    "    # Initialize Network\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    init_x = jnp.zeros((1,) + env.observation_shape)\n",
    "    params = forward.init(_rng, init_x)\n",
    "    opt_state = optimizer.init(params=params)\n",
    "\n",
    "    # Initialize Update Function\n",
    "    if mode == \"ppo\":\n",
    "        _update_step = make_update_fn(with_drift=False, with_lifetime=False)\n",
    "    elif mode == \"dpo\":\n",
    "        _update_step = make_update_fn(with_drift=True, with_lifetime=False)\n",
    "    elif mode == \"lpo\":\n",
    "        _update_step = make_update_fn(with_drift=True, with_lifetime=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "    jitted_update_step = jax.jit(_update_step)\n",
    "\n",
    "    # Initialize Environment\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, args.num_envs)\n",
    "    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (params, opt_state, env_state, env_state.observation, _rng, 0)  # step_count = 0\n",
    "\n",
    "    # Warm-Up\n",
    "    _, _ = jitted_update_step(runner_state)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    # Initial Evaluation\n",
    "    et = time.time()  # exclude evaluation time\n",
    "    tt += et - st\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    eval_R = evaluate(runner_state[0], _rng)\n",
    "    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "    print(log)\n",
    "    wandb.log(log)\n",
    "    st = time.time()\n",
    "\n",
    "    for i in range(num_updates):\n",
    "        runner_state, loss_info = jitted_update_step(runner_state)\n",
    "        steps += args.num_envs * args.num_steps\n",
    "\n",
    "        # Evaluation\n",
    "        et = time.time()  # exclude evaluation time\n",
    "        tt += et - st\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        eval_R = evaluate(runner_state[0], _rng)\n",
    "        log = {\n",
    "            \"sec\": tt,\n",
    "            f\"{args.env_name}/eval_R\": float(eval_R),\n",
    "            \"steps\": steps,\n",
    "            f\"{args.env_name}/loss_actor\": float(loss_info[1]),\n",
    "            f\"{args.env_name}/loss_critic\": float(loss_info[0]),\n",
    "        }\n",
    "        print(log)\n",
    "        wandb.log(log)\n",
    "        st = time.time()\n",
    "\n",
    "    return runner_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args.dict())\n",
    "\n",
    "    # RNG Initialization\n",
    "    rng = jax.random.PRNGKey(args.seed)\n",
    "\n",
    "    # Train PPO\n",
    "    print(\"Training PPO...\")\n",
    "    train(rng, mode=\"ppo\")\n",
    "\n",
    "    if args.save_model:\n",
    "        with open(f\"{args.env_name}-seed={args.seed}.ckpt\", \"wb\") as f:\n",
    "            pickle.dump(out[0], f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
