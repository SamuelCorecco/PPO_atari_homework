{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install minatar\n!pip install dm-haiku\n!pip install distrax\n!pip install pgx\n!pip install omegaconf\n!pip install learn2learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T16:33:16.242986Z","iopub.execute_input":"2024-11-25T16:33:16.246193Z","iopub.status.idle":"2024-11-25T16:34:39.719710Z","shell.execute_reply.started":"2024-11-25T16:33:16.246136Z","shell.execute_reply":"2024-11-25T16:34:39.718750Z"}},"outputs":[{"name":"stdout","text":"Collecting minatar\n  Downloading MinAtar-1.0.15-py3-none-any.whl.metadata (685 bytes)\nRequirement already satisfied: cycler>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from minatar) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from minatar) (1.4.5)\nRequirement already satisfied: matplotlib>=3.0.3 in /opt/conda/lib/python3.10/site-packages (from minatar) (3.7.5)\nRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from minatar) (1.26.4)\nRequirement already satisfied: pandas>=0.24.2 in /opt/conda/lib/python3.10/site-packages (from minatar) (2.2.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from minatar) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from minatar) (2.9.0.post0)\nRequirement already satisfied: pytz>=2018.9 in /opt/conda/lib/python3.10/site-packages (from minatar) (2024.1)\nRequirement already satisfied: scipy>=1.2.1 in /opt/conda/lib/python3.10/site-packages (from minatar) (1.14.1)\nRequirement already satisfied: seaborn>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from minatar) (0.12.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from minatar) (1.16.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.3->minatar) (1.2.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.3->minatar) (4.53.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.3->minatar) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.3->minatar) (10.3.0)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.2->minatar) (2024.1)\nDownloading MinAtar-1.0.15-py3-none-any.whl (16 kB)\nInstalling collected packages: minatar\nSuccessfully installed minatar-1.0.15\nCollecting dm-haiku\n  Downloading dm_haiku-0.0.13-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: absl-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from dm-haiku) (1.4.0)\nCollecting jmp>=0.0.2 (from dm-haiku)\n  Downloading jmp-0.0.4-py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from dm-haiku) (1.26.4)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from dm-haiku) (0.9.0)\nDownloading dm_haiku-0.0.13-py3-none-any.whl (373 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.9/373.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading jmp-0.0.4-py3-none-any.whl (18 kB)\nInstalling collected packages: jmp, dm-haiku\nSuccessfully installed dm-haiku-0.0.13 jmp-0.0.4\nCollecting distrax\n  Downloading distrax-0.1.5-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: absl-py>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from distrax) (1.4.0)\nRequirement already satisfied: chex>=0.1.8 in /opt/conda/lib/python3.10/site-packages (from distrax) (0.1.86)\nRequirement already satisfied: jax>=0.1.55 in /opt/conda/lib/python3.10/site-packages (from distrax) (0.4.26)\nRequirement already satisfied: jaxlib>=0.1.67 in /opt/conda/lib/python3.10/site-packages (from distrax) (0.4.26.dev20240620)\nRequirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from distrax) (1.26.4)\nRequirement already satisfied: tensorflow-probability>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from distrax) (0.24.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from chex>=0.1.8->distrax) (4.12.2)\nRequirement already satisfied: toolz>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chex>=0.1.8->distrax) (0.12.1)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.1.55->distrax) (0.3.2)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax>=0.1.55->distrax) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax>=0.1.55->distrax) (1.14.1)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability>=0.15.0->distrax) (1.16.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability>=0.15.0->distrax) (5.1.1)\nRequirement already satisfied: cloudpickle>=1.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability>=0.15.0->distrax) (3.0.0)\nRequirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability>=0.15.0->distrax) (0.5.4)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability>=0.15.0->distrax) (0.1.8)\nDownloading distrax-0.1.5-py3-none-any.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: distrax\nSuccessfully installed distrax-0.1.5\nCollecting pgx\n  Downloading pgx-2.4.2-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: jax>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pgx) (0.4.26)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pgx) (4.12.2)\nCollecting svgwrite (from pgx)\n  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.6->pgx) (0.3.2)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.6->pgx) (1.26.4)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.6->pgx) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.6->pgx) (1.14.1)\nDownloading pgx-2.4.2-py3-none-any.whl (435 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.5/435.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: svgwrite, pgx\nSuccessfully installed pgx-2.4.2 svgwrite-1.4.3\nCollecting omegaconf\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf) (6.0.2)\nDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=40fb01b557e826a13dfb499116edb8b148acee04946d50aba95cf6638067955c\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, omegaconf\nSuccessfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\nCollecting learn2learn\n  Downloading learn2learn-0.2.0.tar.gz (7.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.10/site-packages (from learn2learn) (1.26.4)\nRequirement already satisfied: gym>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from learn2learn) (0.26.2)\nRequirement already satisfied: torch>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from learn2learn) (2.4.0)\nRequirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from learn2learn) (0.19.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from learn2learn) (1.14.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from learn2learn) (2.32.3)\nCollecting gsutil (from learn2learn)\n  Downloading gsutil-5.31.tar.gz (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from learn2learn) (4.66.4)\nCollecting qpth>=0.0.15 (from learn2learn)\n  Downloading qpth-0.0.18.tar.gz (16 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym>=0.14.0->learn2learn) (3.0.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym>=0.14.0->learn2learn) (0.0.8)\nCollecting cvxpy>=1.1.0 (from qpth>=0.0.15->learn2learn)\n  Downloading cvxpy-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (2024.6.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.3.0->learn2learn) (10.3.0)\nCollecting argcomplete>=1.9.4 (from gsutil->learn2learn)\n  Downloading argcomplete-3.5.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: crcmod>=1.7 in /opt/conda/lib/python3.10/site-packages (from gsutil->learn2learn) (1.7)\nRequirement already satisfied: fasteners>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from gsutil->learn2learn) (0.19)\nCollecting gcs-oauth2-boto-plugin>=3.2 (from gsutil->learn2learn)\n  Downloading gcs-oauth2-boto-plugin-3.2.tar.gz (22 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting google-apitools>=0.5.32 (from gsutil->learn2learn)\n  Downloading google_apitools-0.5.32-py3-none-any.whl.metadata (2.3 kB)\nCollecting httplib2==0.20.4 (from gsutil->learn2learn)\n  Downloading httplib2-0.20.4-py3-none-any.whl.metadata (2.5 kB)\nCollecting google-reauth>=0.1.0 (from gsutil->learn2learn)\n  Downloading google_reauth-0.1.1-py2.py3-none-any.whl.metadata (2.6 kB)\nCollecting monotonic>=1.4 (from gsutil->learn2learn)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: pyOpenSSL>=0.13 in /opt/conda/lib/python3.10/site-packages (from gsutil->learn2learn) (24.0.0)\nCollecting retry_decorator>=1.0.0 (from gsutil->learn2learn)\n  Downloading retry_decorator-1.1.1.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from gsutil->learn2learn) (1.16.0)\nCollecting google-auth==2.17.0 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n  Downloading google_auth-2.17.0-py2.py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: google-auth-httplib2>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from gsutil->learn2learn) (0.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (4.9)\nRequirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in /opt/conda/lib/python3.10/site-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (3.9.5)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2==0.20.4->gsutil->learn2learn) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->learn2learn) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->learn2learn) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->learn2learn) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->learn2learn) (2024.8.30)\nCollecting osqp>=0.6.2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n  Downloading osqp-0.6.7.post3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\nCollecting clarabel>=0.5.0 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n  Downloading clarabel-0.9.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nCollecting scs>=3.2.4.post1 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n  Downloading scs-3.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\nCollecting rsa<5,>=3.1.4 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\nCollecting boto>=2.29.1 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn)\n  Downloading boto-2.49.0-py2.py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: oauth2client>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn) (4.1.3)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (0.6.0)\nRequirement already satisfied: pyu2f in /opt/conda/lib/python3.10/site-packages (from google-reauth>=0.1.0->gsutil->learn2learn) (0.1.5)\nRequirement already satisfied: cryptography<43,>=41.0.5 in /opt/conda/lib/python3.10/site-packages (from pyOpenSSL>=0.13->gsutil->learn2learn) (42.0.8)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.1.0->learn2learn) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.1.0->learn2learn) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (4.0.3)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography<43,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn) (1.16.0)\nCollecting qdldl (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n  Downloading qdldl-0.1.7.post4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography<43,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn) (2.22)\nDownloading google_auth-2.17.0-py2.py3-none-any.whl (178 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.1/178.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading argcomplete-3.5.1-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cvxpy-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\nDownloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_reauth-0.1.1-py2.py3-none-any.whl (17 kB)\nDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading clarabel-0.9.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading osqp-0.6.7.post3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.5/297.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scs-3.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading qdldl-0.1.7.post4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: learn2learn, qpth, gsutil, gcs-oauth2-boto-plugin, retry_decorator\n  Building wheel for learn2learn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for learn2learn: filename=learn2learn-0.2.0-cp310-cp310-linux_x86_64.whl size=402378 sha256=7a24870b317674c1cc56a38797df93f1230035f8beaa9f4f23d804496a6ca394\n  Stored in directory: /root/.cache/pip/wheels/89/2c/13/c538cd229cdfc6c15a9d3cf64d2bb8220e205ea0f63ecb5fbe\n  Building wheel for qpth (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for qpth: filename=qpth-0.0.18-py3-none-any.whl size=19548 sha256=62d6a56cfd5dc5bab1ad98f05840c088aafbb570efe9ac56c765f36bab4570db\n  Stored in directory: /root/.cache/pip/wheels/7b/80/c0/2b553cc21315757f3e03846eb747795bf2a57d15bc14ef80cf\n  Building wheel for gsutil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gsutil: filename=gsutil-5.31-py3-none-any.whl size=3789446 sha256=7323469832936d547c8e147ad84f87ba6063d05fcfec077114b138f3da0f3a03\n  Stored in directory: /root/.cache/pip/wheels/99/ae/9a/f1c62d87bc41548a202b3d28018141dbd1e70e39ddcdc00de4\n  Building wheel for gcs-oauth2-boto-plugin (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gcs-oauth2-boto-plugin: filename=gcs_oauth2_boto_plugin-3.2-py3-none-any.whl size=24469 sha256=5d78dd8e80c18fae28235a9fa027b1803749bc176644599e16970c366cc85cee\n  Stored in directory: /root/.cache/pip/wheels/71/7a/33/4cc4d6af226ef2e5092b72e7a0b7bc49b42467c928dbb191d3\n  Building wheel for retry_decorator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for retry_decorator: filename=retry_decorator-1.1.1-py2.py3-none-any.whl size=3641 sha256=aac60325dd25f36449f231f06950afbbd47f1d7e79db6fe241af2e8dd0638c66\n  Stored in directory: /root/.cache/pip/wheels/dd/ac/77/8c54eac0d373d9eacfbe42599710c9bf91b4c5985297f6922a\nSuccessfully built learn2learn qpth gsutil gcs-oauth2-boto-plugin retry_decorator\nInstalling collected packages: retry_decorator, monotonic, boto, rsa, httplib2, argcomplete, scs, qdldl, google-reauth, google-auth, clarabel, osqp, google-apitools, gcs-oauth2-boto-plugin, cvxpy, qpth, gsutil, learn2learn\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.9\n    Uninstalling rsa-4.9:\n      Successfully uninstalled rsa-4.9\n  Attempting uninstall: httplib2\n    Found existing installation: httplib2 0.21.0\n    Uninstalling httplib2-0.21.0:\n      Successfully uninstalled httplib2-0.21.0\n  Attempting uninstall: google-auth\n    Found existing installation: google-auth 2.30.0\n    Uninstalling google-auth-2.30.0:\n      Successfully uninstalled google-auth-2.30.0\n  Attempting uninstall: google-apitools\n    Found existing installation: google-apitools 0.5.31\n    Uninstalling google-apitools-0.5.31:\n      Successfully uninstalled google-apitools-0.5.31\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.5 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed argcomplete-3.5.1 boto-2.49.0 clarabel-0.9.0 cvxpy-1.6.0 gcs-oauth2-boto-plugin-3.2 google-apitools-0.5.32 google-auth-2.17.0 google-reauth-0.1.1 gsutil-5.31 httplib2-0.20.4 learn2learn-0.2.0 monotonic-1.6 osqp-0.6.7.post3 qdldl-0.1.7.post4 qpth-0.0.18 retry_decorator-1.1.1 rsa-4.7.2 scs-3.2.7\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport sys\nimport jax\nimport jax.numpy as jnp\nimport haiku as hk\nimport optax\nfrom typing import NamedTuple, Literal\nimport distrax\nimport pgx\nfrom pgx.experimental import auto_reset\nimport time\n\nimport pickle\nfrom omegaconf import OmegaConf\nfrom pydantic import BaseModel\nimport wandb\nimport learn2learn as l2l\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nimport numpy as np\nimport optax\nfrom flax.linen.initializers import constant, orthogonal\nfrom typing import Sequence, NamedTuple, Any\nfrom flax.training.train_state import TrainState\nimport distrax\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T16:35:10.602659Z","iopub.execute_input":"2024-11-25T16:35:10.603021Z","iopub.status.idle":"2024-11-25T16:35:31.968419Z","shell.execute_reply.started":"2024-11-25T16:35:10.602987Z","shell.execute_reply":"2024-11-25T16:35:31.967729Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class PPOConfig(BaseModel):\n    env_name: Literal[\n        \"minatar-breakout\",\n        \"minatar-freeway\",\n        \"minatar-space_invaders\",\n        \"minatar-asterix\",\n        \"minatar-seaquest\",\n    ] = \"minatar-space_invaders\"  # Focus on space_invaders\n    seed: int = 0\n    lr: float = 5e-3  # Updated Learning Rate\n    num_envs: int = 64  # Updated number of environments\n    num_eval_envs: int = 100\n    num_steps: int = 128  # Unroll length\n    total_timesteps: int = int(1e7)  # Updated number of timesteps\n    update_epochs: int = 4  # Updated number of update epochs\n    minibatch_size: int = 4096  # This depends on your choice of minibatches\n    gamma: float = 0.99  # Updated gamma\n    gae_lambda: float = 0.95\n    clip_eps: float = 0.2\n    ent_coef: float = 0.01  # Updated entropy coefficient\n    vf_coef: float = 0.5\n    max_grad_norm: float = 0.5  # Updated max grad norm\n    wandb_entity: str = \"nonarruginitocalamarodiferro-usi\"  # Your wandb entity\n    wandb_project: str = \"pgx-minatar-ppo\"  # Your wandb project\n    save_model: bool = False\n\n    class Config:\n        extra = \"forbid\"\n\n\n# Initialize the PPOConfig manually with these hyperparameters\nargs = PPOConfig(\n    env_name=\"minatar-space_invaders\",  # Can be adjusted to other MinAtar environments if needed\n    seed=0,\n    lr=1e-3,\n    num_envs=64,\n    num_eval_envs=100,\n    num_steps=128,\n    total_timesteps=int(1e7),  # 10 million timesteps\n    update_epochs=4,\n    minibatch_size=4096,  # Adjusted as per the batch size and minibatches\n    gamma=0.99,\n    gae_lambda=0.90,\n    clip_eps=0.25,\n    ent_coef=0.005,  # Entropy coefficient set to 0.01\n    vf_coef=0.5,\n    max_grad_norm=0.5,  # Max grad norm set to 0.5\n    wandb_entity=\"nonarruginitocalamarodiferro-usi\",  # wandb entity\n    wandb_project=\"pgx-minatar-ppo\",  # wandb project\n    save_model=False  # Set to True if you want to save the model after training\n)\n\nprint(args)  # To verify your updated configuration\n\n# Initialize environment\nenv = pgx.make(str(args.env_name))\n\nnum_updates = args.total_timesteps // args.num_envs // args.num_steps\nnum_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n\n\n\n\nclass ActorCritic(hk.Module):\n    def __init__(self, num_actions, activation=\"tanh\"):\n        super().__init__()\n        self.num_actions = num_actions\n        self.activation = activation\n        assert activation in [\"relu\", \"tanh\"]\n\n    def __call__(self, x):\n        x = x.astype(jnp.float32)\n        if self.activation == \"relu\":\n            activation = jax.nn.relu\n        else:\n            activation = jax.nn.tanh\n        x = hk.Conv2D(32, kernel_shape=2)(x)\n        x = jax.nn.relu(x)\n        x = hk.avg_pool(x, window_shape=(2, 2),\n                        strides=(2, 2), padding=\"VALID\")\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = hk.Linear(64)(x)\n        x = jax.nn.relu(x)\n        actor_mean = hk.Linear(64)(x)\n        actor_mean = activation(actor_mean)\n        actor_mean = hk.Linear(64)(actor_mean)\n        actor_mean = activation(actor_mean)\n        actor_mean = hk.Linear(self.num_actions)(actor_mean)\n\n        critic = hk.Linear(64)(x)\n        critic = activation(critic)\n        critic = hk.Linear(64)(critic)\n        critic = activation(critic)\n        critic = hk.Linear(1)(critic)\n\n        return actor_mean, jnp.squeeze(critic, axis=-1)\n\n\ndef forward_fn(x, is_eval=False):\n    net = ActorCritic(env.num_actions, activation=\"tanh\")\n    logits, value = net(x)\n    return logits, value\n\n\nforward = hk.without_apply_rng(hk.transform(forward_fn))\n\n\noptimizer = optax.chain(optax.clip_by_global_norm(\n    args.max_grad_norm), optax.adam(args.lr, eps=1e-5))\n\n\nclass Transition(NamedTuple):\n    done: jnp.ndarray\n    action: jnp.ndarray\n    value: jnp.ndarray\n    reward: jnp.ndarray\n    log_prob: jnp.ndarray\n    obs: jnp.ndarray\n\n\ndef make_update_fn():\n    # TRAIN LOOP\n    def _update_step(runner_state):\n        # COLLECT TRAJECTORIES\n        step_fn = jax.vmap(auto_reset(env.step, env.init))\n\n        def _env_step(runner_state, unused):\n            params, opt_state, env_state, last_obs, rng = runner_state\n            # SELECT ACTION\n            rng, _rng = jax.random.split(rng)\n            logits, value = forward.apply(params, last_obs)\n            pi = distrax.Categorical(logits=logits)\n            action = pi.sample(seed=_rng)\n            log_prob = pi.log_prob(action)\n\n            # STEP ENV\n            rng, _rng = jax.random.split(rng)\n            keys = jax.random.split(_rng, env_state.observation.shape[0])\n            env_state = step_fn(env_state, action, keys)\n            transition = Transition(\n                env_state.terminated,\n                action,\n                value,\n                jnp.squeeze(env_state.rewards),\n                log_prob,\n                last_obs\n            )\n            runner_state = (params, opt_state, env_state,\n                            env_state.observation, rng)\n            return runner_state, transition\n\n        runner_state, traj_batch = jax.lax.scan(\n            _env_step, runner_state, None, args.num_steps\n        )\n\n        # CALCULATE ADVANTAGE\n        params, opt_state, env_state, last_obs, rng = runner_state\n        _, last_val = forward.apply(params, last_obs)\n\n        def _calculate_gae(traj_batch, last_val):\n            def _get_advantages(gae_and_next_value, transition):\n                gae, next_value = gae_and_next_value\n                done, value, reward = (\n                    transition.done,\n                    transition.value,\n                    transition.reward,\n                )\n                delta = reward + args.gamma * next_value * (1 - done) - value\n                gae = (\n                    delta\n                    + args.gamma * args.gae_lambda * (1 - done) * gae\n                )\n                return (gae, value), gae\n\n            _, advantages = jax.lax.scan(\n                _get_advantages,\n                (jnp.zeros_like(last_val), last_val),\n                traj_batch,\n                reverse=True,\n                unroll=16,\n            )\n            return advantages, advantages + traj_batch.value\n\n        advantages, targets = _calculate_gae(traj_batch, last_val)\n\n        # UPDATE NETWORK\n        def _update_epoch(update_state, unused):\n            def _update_minbatch(tup, batch_info):\n                params, opt_state = tup\n                traj_batch, advantages, targets = batch_info\n\n                def _loss_fn(params, traj_batch, gae, targets):\n                    # RERUN NETWORK\n                    logits, value = forward.apply(params, traj_batch.obs)\n                    pi = distrax.Categorical(logits=logits)\n                    log_prob = pi.log_prob(traj_batch.action)\n\n                    # CALCULATE VALUE LOSS\n                    value_pred_clipped = traj_batch.value + (\n                        value - traj_batch.value\n                    ).clip(-args.clip_eps, args.clip_eps)\n                    value_losses = jnp.square(value - targets)\n                    value_losses_clipped = jnp.square(\n                        value_pred_clipped - targets)\n                    value_loss = (\n                        0.5 * jnp.maximum(value_losses,\n                                          value_losses_clipped).mean()\n                    )\n\n                    # CALCULATE ACTOR LOSS\n                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n                    loss_actor1 = ratio * gae\n                    loss_actor2 = (\n                        jnp.clip(\n                            ratio,\n                            1.0 - args.clip_eps,\n                            1.0 + args.clip_eps,\n                        )\n                        * gae\n                    )\n                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n                    loss_actor = loss_actor.mean()\n                    entropy = pi.entropy().mean()\n\n                    total_loss = (\n                        loss_actor\n                        + args.vf_coef * value_loss\n                        - args.ent_coef * entropy\n                    )\n                    return total_loss, (value_loss, loss_actor, entropy)\n\n                grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n                total_loss, grads = grad_fn(\n                    params, traj_batch, advantages, targets)\n                updates, opt_state = optimizer.update(grads, opt_state)\n                params = optax.apply_updates(params, updates)\n                return (params, opt_state), total_loss\n\n            params, opt_state, traj_batch, advantages, targets, rng = update_state\n            rng, _rng = jax.random.split(rng)\n            batch_size = args.minibatch_size * num_minibatches\n            assert (\n                batch_size == args.num_steps * args.num_envs\n            ), \"batch size must be equal to number of steps * number of envs\"\n            permutation = jax.random.permutation(_rng, batch_size)\n            batch = (traj_batch, advantages, targets)\n            batch = jax.tree_util.tree_map(\n                lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n            )\n            shuffled_batch = jax.tree_util.tree_map(\n                lambda x: jnp.take(x, permutation, axis=0), batch\n            )\n            minibatches = jax.tree_util.tree_map(\n                lambda x: jnp.reshape(\n                    x, [num_minibatches, -1] + list(x.shape[1:])\n                ),\n                shuffled_batch,\n            )\n            (params, opt_state),  total_loss = jax.lax.scan(\n                _update_minbatch, (params, opt_state), minibatches\n            )\n            update_state = (params, opt_state, traj_batch,\n                            advantages, targets, rng)\n            return update_state, total_loss\n\n        update_state = (params, opt_state, traj_batch,\n                        advantages, targets, rng)\n        update_state, loss_info = jax.lax.scan(\n            _update_epoch, update_state, None, args.update_epochs\n        )\n        params, opt_state, _, _, _, rng = update_state\n\n        runner_state = (params, opt_state, env_state, last_obs, rng)\n        return runner_state, loss_info\n    return _update_step\n\n\n@jax.jit\ndef evaluate(params, rng_key):\n    step_fn = jax.vmap(env.step)\n    rng_key, sub_key = jax.random.split(rng_key)\n    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n    state = jax.vmap(env.init)(subkeys)\n    R = jnp.zeros_like(state.rewards)\n\n    def cond_fn(tup):\n        state, _, _ = tup\n        return ~state.terminated.all()\n\n    def loop_fn(tup):\n        state, R, rng_key = tup\n        logits, value = forward.apply(params, state.observation)\n        # action = logits.argmax(axis=-1)\n        pi = distrax.Categorical(logits=logits)\n        rng_key, _rng = jax.random.split(rng_key)\n        action = pi.sample(seed=_rng)\n        rng_key, _rng = jax.random.split(rng_key)\n        keys = jax.random.split(_rng, state.observation.shape[0])\n        state = step_fn(state, action, keys)\n        return state, R + state.rewards, rng_key\n    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n    return R.mean()\n\n\ndef train(rng):\n    tt = 0\n    st = time.time()\n    # INIT NETWORK\n    rng, _rng = jax.random.split(rng)\n    init_x = jnp.zeros((1, ) + env.observation_shape)\n    params = forward.init(_rng, init_x)\n    opt_state = optimizer.init(params=params)\n\n    # INIT UPDATE FUNCTION\n    _update_step = make_update_fn()\n    jitted_update_step = jax.jit(_update_step)\n\n    # INIT ENV\n    rng, _rng = jax.random.split(rng)\n    reset_rng = jax.random.split(_rng, args.num_envs)\n    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n\n    rng, _rng = jax.random.split(rng)\n    runner_state = (params, opt_state, env_state, env_state.observation, _rng)\n\n    # warm up\n    _, _ = jitted_update_step(runner_state)\n\n    steps = 0\n\n    # initial evaluation\n    et = time.time()  # exclude evaluation time\n    tt += et - st\n    rng, _rng = jax.random.split(rng)\n    eval_R = evaluate(runner_state[0], _rng)\n    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n    print(log)\n    wandb.log(log)\n    st = time.time()\n\n    for i in range(num_updates):\n        runner_state, loss_info = jitted_update_step(runner_state)\n        steps += args.num_envs * args.num_steps\n\n        # evaluation\n        et = time.time()  # exclude evaluation time\n        tt += et - st\n        rng, _rng = jax.random.split(rng)\n        eval_R = evaluate(runner_state[0], _rng)\n        log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n        print(log)\n        wandb.log(log)\n        st = time.time()\n\n    return runner_state\n\nif __name__ == \"__main__\":\n    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args.dict())\n    rng = jax.random.PRNGKey(args.seed)\n    out = train(rng)\n    if args.save_model:\n        with open(f\"{args.env_name}-seed={args.seed}.ckpt\", \"wb\") as f:\n            pickle.dump(out[0], f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:44:21.036517Z","iopub.execute_input":"2024-11-24T18:44:21.036898Z","iopub.status.idle":"2024-11-24T18:44:21.079674Z","shell.execute_reply.started":"2024-11-24T18:44:21.036869Z","shell.execute_reply":"2024-11-24T18:44:21.078908Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"env_name='minatar-space_invaders' seed=0 lr=0.001 num_envs=64 num_eval_envs=100 num_steps=128 total_timesteps=10000000 update_epochs=4 minibatch_size=4096 gamma=0.99 gae_lambda=0.9 clip_eps=0.25 ent_coef=0.005 vf_coef=0.5 max_grad_norm=0.5 wandb_entity='nonarruginitocalamarodiferro-usi' wandb_project='pgx-minatar-ppo' save_model=False\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"class DPOConfig(BaseModel):\n    env_name: Literal[\n        \"minatar-breakout\",\n        \"minatar-freeway\",\n        \"minatar-space_invaders\",\n        \"minatar-asterix\",\n        \"minatar-seaquest\",\n    ] = \"minatar-space_invaders\"  # Ambiente di default\n    seed: int = 0  # Random seed\n    lr: float = 5e-3  # Learning rate aggiornato\n    num_envs: int = 64  # Numero di ambienti paralleli\n    num_eval_envs: int = 100  # Numero di ambienti di valutazione\n    num_steps: int = 128  # Unroll length\n    total_timesteps: int = int(1e7)  # Totale timesteps: 10 milioni\n    update_epochs: int = 4  # Numero di epoche per aggiornare\n    minibatch_size: int = 8192  # Calcolato come num_envs * num_steps // num_minibatches\n    num_minibatches: int = 8  # Numero di minibatch\n    gamma: float = 0.99  # Fattore di sconto\n    gae_lambda: float = 0.95  # Lambda per GAE\n    clip_eps: float = 0.25  # Clipping ratio\n    ent_coef: float = 0.01  # Entropy coefficient aggiornato\n    vf_coef: float = 0.5  # Coefficiente per il valore (critic)\n    max_grad_norm: float = 8.0  # Max grad norm per LPO/DPO\n    dpo_alpha: float = 2.0  # Alpha per il drift di DPO\n    dpo_beta: float = 0.6  # Beta per il drift di DPO\n    wandb_entity: str = \"nonarruginitocalamarodiferro-usi\"  # wandb entity\n    wandb_project: str = \"pgx-minatar-dpo\"  # wandb project aggiornato\n    save_model: bool = False  # Se salvare il modello o meno\n\n    class Config:\n        extra = \"forbid\"  # Impedisce parametri extra nella configurazione\n\nargs = DPOConfig(\n    env_name=\"minatar-space_invaders\",  # Ambiente target\n    seed=0,  # Seed random per riproducibilità\n    lr=5e-3,  # Learning rate aggiornato\n    num_envs=64,  # Numero di ambienti paralleli\n    num_eval_envs=100,  # Numero di ambienti di valutazione\n    num_steps=128,  # Unroll length\n    total_timesteps=int(1e7),  # 10 milioni di timesteps\n    update_epochs=4,  # Numero di epoche per aggiornare\n    num_minibatches=8,  # Numero di minibatch\n    gamma=0.99,  # Fattore di sconto\n    gae_lambda=0.95,  # Lambda per GAE\n    clip_eps=0.20,  # Clipping ratio\n    ent_coef=0.01,  # Entropy coefficient aggiornato\n    vf_coef=0.5,  # Coefficiente per il valore (critic)\n    max_grad_norm=8.0,  # Max grad norm per LPO/DPO\n    dpo_alpha=2.0,  # Parametro alpha per il drift di DPO\n    dpo_beta=1.2,  # Parametro beta per il drift di DPO\n    wandb_entity=\"nonarruginitocalamarodiferro-usi\",  # Nome entità per wandb\n    wandb_project=\"pgx-minatar-ppo\",  # Nome progetto per wandb\n    save_model=False  # Se salvare il modello o meno\n)\n\nprint(args)  # To verify your updated configuration\n\n# Initialize environment\nenv = pgx.make(str(args.env_name))\n\nnum_updates = args.total_timesteps // args.num_envs // args.num_steps\nnum_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n\n\n\n\nclass ActorCritic(hk.Module):\n    def __init__(self, num_actions, activation=\"tanh\"):\n        super().__init__()\n        self.num_actions = num_actions\n        self.activation = activation\n        assert activation in [\"relu\", \"tanh\"]\n\n    def __call__(self, x):\n        x = x.astype(jnp.float32)\n        if self.activation == \"relu\":\n            activation = jax.nn.relu\n        else:\n            activation = jax.nn.tanh\n        x = hk.Conv2D(32, kernel_shape=2)(x)\n        x = jax.nn.relu(x)\n        x = hk.avg_pool(x, window_shape=(2, 2),\n                        strides=(2, 2), padding=\"VALID\")\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = hk.Linear(64)(x)\n        x = jax.nn.relu(x)\n        actor_mean = hk.Linear(64)(x)\n        actor_mean = activation(actor_mean)\n        actor_mean = hk.Linear(64)(actor_mean)\n        actor_mean = activation(actor_mean)\n        actor_mean = hk.Linear(self.num_actions)(actor_mean)\n\n        critic = hk.Linear(64)(x)\n        critic = activation(critic)\n        critic = hk.Linear(64)(critic)\n        critic = activation(critic)\n        critic = hk.Linear(1)(critic)\n\n        return actor_mean, jnp.squeeze(critic, axis=-1)\n\n\ndef forward_fn(x, is_eval=False):\n    net = ActorCritic(env.num_actions, activation=\"tanh\")\n    logits, value = net(x)\n    return logits, value\n\n\nforward = hk.without_apply_rng(hk.transform(forward_fn))\n\n\noptimizer = optax.chain(optax.clip_by_global_norm(\n    args.max_grad_norm), optax.adam(args.lr, eps=1e-5))\n\n\nclass Transition(NamedTuple):\n    done: jnp.ndarray\n    action: jnp.ndarray\n    value: jnp.ndarray\n    reward: jnp.ndarray\n    log_prob: jnp.ndarray\n    obs: jnp.ndarray\n\n\ndef make_update_fn():\n    # TRAIN LOOP\n    def _update_step(runner_state):\n        # COLLECT TRAJECTORIES\n        step_fn = jax.vmap(auto_reset(env.step, env.init))\n\n        def _env_step(runner_state, unused):\n            params, opt_state, env_state, last_obs, rng = runner_state\n            # SELECT ACTION\n            rng, _rng = jax.random.split(rng)\n            logits, value = forward.apply(params, last_obs)\n            pi = distrax.Categorical(logits=logits)\n            action = pi.sample(seed=_rng)\n            log_prob = pi.log_prob(action)\n\n            # STEP ENV\n            rng, _rng = jax.random.split(rng)\n            keys = jax.random.split(_rng, env_state.observation.shape[0])\n            env_state = step_fn(env_state, action, keys)\n            transition = Transition(\n                env_state.terminated,\n                action,\n                value,\n                jnp.squeeze(env_state.rewards),\n                log_prob,\n                last_obs\n            )\n            runner_state = (params, opt_state, env_state,\n                            env_state.observation, rng)\n            return runner_state, transition\n\n        runner_state, traj_batch = jax.lax.scan(\n            _env_step, runner_state, None, args.num_steps\n        )\n\n        # CALCULATE ADVANTAGE\n        params, opt_state, env_state, last_obs, rng = runner_state\n        _, last_val = forward.apply(params, last_obs)\n\n        def _calculate_gae(traj_batch, last_val):\n            def _get_advantages(gae_and_next_value, transition):\n                gae, next_value = gae_and_next_value\n                done, value, reward = (\n                    transition.done,\n                    transition.value,\n                    transition.reward,\n                )\n                delta = reward + args.gamma * next_value * (1 - done) - value\n                gae = (\n                    delta\n                    + args.gamma * args.gae_lambda * (1 - done) * gae\n                )\n                return (gae, value), gae\n\n            _, advantages = jax.lax.scan(\n                _get_advantages,\n                (jnp.zeros_like(last_val), last_val),\n                traj_batch,\n                reverse=True,\n                unroll=16,\n            )\n            return advantages, advantages + traj_batch.value\n\n        advantages, targets = _calculate_gae(traj_batch, last_val)\n\n        # UPDATE NETWORK\n        def _update_epoch(update_state, unused):\n            def _update_minbatch(tup, batch_info):\n                params, opt_state = tup\n                traj_batch, advantages, targets = batch_info\n\n                def _loss_fn(params, traj_batch, gae, targets):\n                    # RERUN NETWORK\n                    logits, value = forward.apply(params, traj_batch.obs)\n                    pi = distrax.Categorical(logits=logits)\n                    log_prob = pi.log_prob(traj_batch.action)\n                \n                    # CALCOLA LA PERDITA DEL VALORE (CRITICO)\n                    value_pred_clipped = traj_batch.value + (\n                        value - traj_batch.value\n                    ).clip(-args.clip_eps, args.clip_eps)\n                    value_losses = jnp.square(value - targets)\n                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n                \n                    # CALCOLA LA PERDITA DELL'ATTORE (DPO)\n                    log_diff = log_prob - traj_batch.log_prob\n                    ratio = jnp.exp(log_diff)\n                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n                \n                    # Differenzia tra vantaggio positivo e negativo\n                    is_pos = (gae >= 0.0).astype(jnp.float32)\n                \n                    # Drift per vantaggi positivi\n                    r1 = ratio - 1.0\n                    drift1 = nn.relu(r1 * gae - args.dpo_alpha * jnp.tanh(r1 * gae / args.dpo_alpha))\n                \n                    # Drift per vantaggi negativi\n                    drift2 = nn.relu(\n                        log_diff * gae - args.dpo_beta * jnp.tanh(log_diff * gae / args.dpo_beta)\n                    )\n                \n                    # Combina i drift in base al segno del vantaggio\n                    drift = drift1 * is_pos + drift2 * (1 - is_pos)\n                \n                    # Calcola la perdita dell'attore con drift\n                    loss_actor = -(ratio * gae - drift).mean()\n                    entropy = pi.entropy().mean()\n                \n                    # PERDITA TOTALE\n                    total_loss = (\n                        loss_actor\n                        + args.vf_coef * value_loss\n                        - args.ent_coef * entropy\n                    )\n                    return total_loss, (value_loss, loss_actor, entropy)\n\n                grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n                total_loss, grads = grad_fn(\n                    params, traj_batch, advantages, targets)\n                updates, opt_state = optimizer.update(grads, opt_state)\n                params = optax.apply_updates(params, updates)\n                return (params, opt_state), total_loss\n\n            params, opt_state, traj_batch, advantages, targets, rng = update_state\n            rng, _rng = jax.random.split(rng)\n            batch_size = args.minibatch_size * num_minibatches\n            assert (\n                batch_size == args.num_steps * args.num_envs\n            ), \"batch size must be equal to number of steps * number of envs\"\n            permutation = jax.random.permutation(_rng, batch_size)\n            batch = (traj_batch, advantages, targets)\n            batch = jax.tree_util.tree_map(\n                lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n            )\n            shuffled_batch = jax.tree_util.tree_map(\n                lambda x: jnp.take(x, permutation, axis=0), batch\n            )\n            minibatches = jax.tree_util.tree_map(\n                lambda x: jnp.reshape(\n                    x, [num_minibatches, -1] + list(x.shape[1:])\n                ),\n                shuffled_batch,\n            )\n            (params, opt_state),  total_loss = jax.lax.scan(\n                _update_minbatch, (params, opt_state), minibatches\n            )\n            update_state = (params, opt_state, traj_batch,\n                            advantages, targets, rng)\n            return update_state, total_loss\n\n        update_state = (params, opt_state, traj_batch,\n                        advantages, targets, rng)\n        update_state, loss_info = jax.lax.scan(\n            _update_epoch, update_state, None, args.update_epochs\n        )\n        params, opt_state, _, _, _, rng = update_state\n\n        runner_state = (params, opt_state, env_state, last_obs, rng)\n        return runner_state, loss_info\n    return _update_step\n\n\n@jax.jit\ndef evaluate(params, rng_key):\n    step_fn = jax.vmap(env.step)\n    rng_key, sub_key = jax.random.split(rng_key)\n    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n    state = jax.vmap(env.init)(subkeys)\n    R = jnp.zeros_like(state.rewards)\n\n    def cond_fn(tup):\n        state, _, _ = tup\n        return ~state.terminated.all()\n\n    def loop_fn(tup):\n        state, R, rng_key = tup\n        logits, value = forward.apply(params, state.observation)\n        # action = logits.argmax(axis=-1)\n        pi = distrax.Categorical(logits=logits)\n        rng_key, _rng = jax.random.split(rng_key)\n        action = pi.sample(seed=_rng)\n        rng_key, _rng = jax.random.split(rng_key)\n        keys = jax.random.split(_rng, state.observation.shape[0])\n        state = step_fn(state, action, keys)\n        return state, R + state.rewards, rng_key\n    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n    return R.mean()\n\n\ndef train(rng):\n    tt = 0\n    st = time.time()\n    # INIT NETWORK\n    rng, _rng = jax.random.split(rng)\n    init_x = jnp.zeros((1, ) + env.observation_shape)\n    params = forward.init(_rng, init_x)\n    opt_state = optimizer.init(params=params)\n\n    # INIT UPDATE FUNCTION\n    _update_step = make_update_fn()\n    jitted_update_step = jax.jit(_update_step)\n\n    # INIT ENV\n    rng, _rng = jax.random.split(rng)\n    reset_rng = jax.random.split(_rng, args.num_envs)\n    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n\n    rng, _rng = jax.random.split(rng)\n    runner_state = (params, opt_state, env_state, env_state.observation, _rng)\n\n    # warm up\n    _, _ = jitted_update_step(runner_state)\n\n    steps = 0\n\n    # initial evaluation\n    et = time.time()  # exclude evaluation time\n    tt += et - st\n    rng, _rng = jax.random.split(rng)\n    eval_R = evaluate(runner_state[0], _rng)\n    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n    print(log)\n    wandb.log(log)\n    st = time.time()\n\n    for i in range(num_updates):\n        runner_state, loss_info = jitted_update_step(runner_state)\n        steps += args.num_envs * args.num_steps\n\n        # evaluation\n        et = time.time()  # exclude evaluation time\n        tt += et - st\n        rng, _rng = jax.random.split(rng)\n        eval_R = evaluate(runner_state[0], _rng)\n        log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n        print(log)\n        wandb.log(log)\n        st = time.time()\n\n    return runner_state\n\nif __name__ == \"__main__\":\n    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args.dict())\n    rng = jax.random.PRNGKey(args.seed)\n    out = train(rng)\n    if args.save_model:\n        with open(f\"{args.env_name}-seed={args.seed}.ckpt\", \"wb\") as f:\n            pickle.dump(out[0], f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T16:41:16.787245Z","iopub.execute_input":"2024-11-25T16:41:16.787556Z"}},"outputs":[{"name":"stdout","text":"env_name='minatar-space_invaders' seed=0 lr=0.005 num_envs=64 num_eval_envs=100 num_steps=128 total_timesteps=10000000 update_epochs=4 minibatch_size=8192 num_minibatches=8 gamma=0.99 gae_lambda=0.95 clip_eps=0.2 ent_coef=0.01 vf_coef=0.5 max_grad_norm=8.0 dpo_alpha=2.0 dpo_beta=1.2 wandb_entity='nonarruginitocalamarodiferro-usi' wandb_project='pgx-minatar-ppo' save_model=False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:xr2hi8i7) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.033 MB of 0.033 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd1e9b483aa4ef4b692cca3dc5da49c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>minatar-space_invaders/eval_R</td><td>▁▁▁▁▁▂▃▄▄▅▅▆▆▆▆▆▆▇▆█▇▇▇█▇▇▇▇▇▇▆▇▆▇▇▆▇▆██</td></tr><tr><td>sec</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>steps</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>minatar-space_invaders/eval_R</td><td>39.32</td></tr><tr><td>sec</td><td>19.58736</td></tr><tr><td>steps</td><td>1376256</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">zany-wind-55</strong> at: <a href='https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo/runs/xr2hi8i7' target=\"_blank\">https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo/runs/xr2hi8i7</a><br/> View project at: <a href='https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo' target=\"_blank\">https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241125_163606-xr2hi8i7/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:xr2hi8i7). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241125_164116-n92fvh78</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo/runs/n92fvh78' target=\"_blank\">hearty-sea-56</a></strong> to <a href='https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo' target=\"_blank\">https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo/runs/n92fvh78' target=\"_blank\">https://wandb.ai/nonarruginitocalamarodiferro-usi/pgx-minatar-ppo/runs/n92fvh78</a>"},"metadata":{}},{"name":"stdout","text":"{'sec': 5.309812068939209, 'minatar-space_invaders/eval_R': 4.349999904632568, 'steps': 0}\n{'sec': 5.399070978164673, 'minatar-space_invaders/eval_R': 4.610000133514404, 'steps': 8192}\n{'sec': 5.4634788036346436, 'minatar-space_invaders/eval_R': 5.089999675750732, 'steps': 16384}\n{'sec': 5.518078327178955, 'minatar-space_invaders/eval_R': 5.559999942779541, 'steps': 24576}\n{'sec': 5.573474884033203, 'minatar-space_invaders/eval_R': 5.699999809265137, 'steps': 32768}\n{'sec': 5.628509044647217, 'minatar-space_invaders/eval_R': 6.009999752044678, 'steps': 40960}\n{'sec': 5.682448148727417, 'minatar-space_invaders/eval_R': 7.349999904632568, 'steps': 49152}\n{'sec': 5.739185810089111, 'minatar-space_invaders/eval_R': 6.869999885559082, 'steps': 57344}\n{'sec': 5.792344570159912, 'minatar-space_invaders/eval_R': 6.909999847412109, 'steps': 65536}\n{'sec': 5.8447425365448, 'minatar-space_invaders/eval_R': 7.989999771118164, 'steps': 73728}\n{'sec': 5.898927450180054, 'minatar-space_invaders/eval_R': 8.399999618530273, 'steps': 81920}\n{'sec': 5.955074071884155, 'minatar-space_invaders/eval_R': 8.970000267028809, 'steps': 90112}\n{'sec': 6.010570526123047, 'minatar-space_invaders/eval_R': 8.5, 'steps': 98304}\n{'sec': 6.068293571472168, 'minatar-space_invaders/eval_R': 9.069999694824219, 'steps': 106496}\n{'sec': 6.122711658477783, 'minatar-space_invaders/eval_R': 10.029999732971191, 'steps': 114688}\n{'sec': 6.179882049560547, 'minatar-space_invaders/eval_R': 10.579999923706055, 'steps': 122880}\n{'sec': 6.235838890075684, 'minatar-space_invaders/eval_R': 10.130000114440918, 'steps': 131072}\n{'sec': 6.295469045639038, 'minatar-space_invaders/eval_R': 10.9399995803833, 'steps': 139264}\n{'sec': 6.355217218399048, 'minatar-space_invaders/eval_R': 11.529999732971191, 'steps': 147456}\n{'sec': 6.41578221321106, 'minatar-space_invaders/eval_R': 11.639999389648438, 'steps': 155648}\n{'sec': 6.481989145278931, 'minatar-space_invaders/eval_R': 11.179999351501465, 'steps': 163840}\n{'sec': 6.541174650192261, 'minatar-space_invaders/eval_R': 10.510000228881836, 'steps': 172032}\n{'sec': 6.607836484909058, 'minatar-space_invaders/eval_R': 11.989999771118164, 'steps': 180224}\n{'sec': 6.669542074203491, 'minatar-space_invaders/eval_R': 13.649999618530273, 'steps': 188416}\n{'sec': 6.7351930141448975, 'minatar-space_invaders/eval_R': 14.589999198913574, 'steps': 196608}\n{'sec': 6.800185441970825, 'minatar-space_invaders/eval_R': 14.309999465942383, 'steps': 204800}\n{'sec': 6.863343000411987, 'minatar-space_invaders/eval_R': 15.229999542236328, 'steps': 212992}\n{'sec': 6.92531681060791, 'minatar-space_invaders/eval_R': 15.609999656677246, 'steps': 221184}\n{'sec': 6.986666440963745, 'minatar-space_invaders/eval_R': 18.43000030517578, 'steps': 229376}\n{'sec': 7.050288677215576, 'minatar-space_invaders/eval_R': 17.770000457763672, 'steps': 237568}\n{'sec': 7.11207389831543, 'minatar-space_invaders/eval_R': 20.619998931884766, 'steps': 245760}\n{'sec': 7.173367261886597, 'minatar-space_invaders/eval_R': 19.739999771118164, 'steps': 253952}\n{'sec': 7.235934734344482, 'minatar-space_invaders/eval_R': 17.75, 'steps': 262144}\n{'sec': 7.298104286193848, 'minatar-space_invaders/eval_R': 19.049999237060547, 'steps': 270336}\n{'sec': 7.359071493148804, 'minatar-space_invaders/eval_R': 19.78999900817871, 'steps': 278528}\n{'sec': 7.422916412353516, 'minatar-space_invaders/eval_R': 18.6299991607666, 'steps': 286720}\n{'sec': 7.484715938568115, 'minatar-space_invaders/eval_R': 22.53999900817871, 'steps': 294912}\n{'sec': 7.546677827835083, 'minatar-space_invaders/eval_R': 23.779998779296875, 'steps': 303104}\n{'sec': 7.607697248458862, 'minatar-space_invaders/eval_R': 22.899999618530273, 'steps': 311296}\n{'sec': 7.6705162525177, 'minatar-space_invaders/eval_R': 21.689998626708984, 'steps': 319488}\n{'sec': 7.732821702957153, 'minatar-space_invaders/eval_R': 25.059999465942383, 'steps': 327680}\n{'sec': 7.793880224227905, 'minatar-space_invaders/eval_R': 27.229999542236328, 'steps': 335872}\n{'sec': 7.855379819869995, 'minatar-space_invaders/eval_R': 29.459999084472656, 'steps': 344064}\n{'sec': 7.916948318481445, 'minatar-space_invaders/eval_R': 26.029998779296875, 'steps': 352256}\n{'sec': 7.978932857513428, 'minatar-space_invaders/eval_R': 28.469999313354492, 'steps': 360448}\n{'sec': 8.041378498077393, 'minatar-space_invaders/eval_R': 26.889999389648438, 'steps': 368640}\n{'sec': 8.1037015914917, 'minatar-space_invaders/eval_R': 27.439998626708984, 'steps': 376832}\n{'sec': 8.165568828582764, 'minatar-space_invaders/eval_R': 25.489999771118164, 'steps': 385024}\n{'sec': 8.22775912284851, 'minatar-space_invaders/eval_R': 25.69999885559082, 'steps': 393216}\n{'sec': 8.290274381637573, 'minatar-space_invaders/eval_R': 27.34000015258789, 'steps': 401408}\n{'sec': 8.35248851776123, 'minatar-space_invaders/eval_R': 27.229999542236328, 'steps': 409600}\n{'sec': 8.415065050125122, 'minatar-space_invaders/eval_R': 28.170000076293945, 'steps': 417792}\n{'sec': 8.476894855499268, 'minatar-space_invaders/eval_R': 28.279998779296875, 'steps': 425984}\n{'sec': 8.537187814712524, 'minatar-space_invaders/eval_R': 27.85999870300293, 'steps': 434176}\n{'sec': 8.600142002105713, 'minatar-space_invaders/eval_R': 28.1299991607666, 'steps': 442368}\n{'sec': 8.661547183990479, 'minatar-space_invaders/eval_R': 28.85999870300293, 'steps': 450560}\n{'sec': 8.723257303237915, 'minatar-space_invaders/eval_R': 29.869998931884766, 'steps': 458752}\n{'sec': 8.784419298171997, 'minatar-space_invaders/eval_R': 28.469999313354492, 'steps': 466944}\n{'sec': 8.845369100570679, 'minatar-space_invaders/eval_R': 28.799999237060547, 'steps': 475136}\n{'sec': 8.907638549804688, 'minatar-space_invaders/eval_R': 27.40999984741211, 'steps': 483328}\n{'sec': 8.969625473022461, 'minatar-space_invaders/eval_R': 29.049999237060547, 'steps': 491520}\n{'sec': 9.031774997711182, 'minatar-space_invaders/eval_R': 25.439998626708984, 'steps': 499712}\n{'sec': 9.095119714736938, 'minatar-space_invaders/eval_R': 26.969999313354492, 'steps': 507904}\n{'sec': 9.158117532730103, 'minatar-space_invaders/eval_R': 27.670000076293945, 'steps': 516096}\n{'sec': 9.218966245651245, 'minatar-space_invaders/eval_R': 27.119998931884766, 'steps': 524288}\n{'sec': 9.28072738647461, 'minatar-space_invaders/eval_R': 25.35999870300293, 'steps': 532480}\n{'sec': 9.343185663223267, 'minatar-space_invaders/eval_R': 29.149999618530273, 'steps': 540672}\n{'sec': 9.405375480651855, 'minatar-space_invaders/eval_R': 26.59000015258789, 'steps': 548864}\n{'sec': 9.467494010925293, 'minatar-space_invaders/eval_R': 32.40999984741211, 'steps': 557056}\n{'sec': 9.529793977737427, 'minatar-space_invaders/eval_R': 30.90999984741211, 'steps': 565248}\n{'sec': 9.59056544303894, 'minatar-space_invaders/eval_R': 33.98999786376953, 'steps': 573440}\n{'sec': 9.65256953239441, 'minatar-space_invaders/eval_R': 31.229999542236328, 'steps': 581632}\n{'sec': 9.714983224868774, 'minatar-space_invaders/eval_R': 32.619998931884766, 'steps': 589824}\n{'sec': 9.778899431228638, 'minatar-space_invaders/eval_R': 33.54999923706055, 'steps': 598016}\n{'sec': 9.841085195541382, 'minatar-space_invaders/eval_R': 34.80999755859375, 'steps': 606208}\n{'sec': 9.903594017028809, 'minatar-space_invaders/eval_R': 32.06999969482422, 'steps': 614400}\n{'sec': 9.966603755950928, 'minatar-space_invaders/eval_R': 35.209999084472656, 'steps': 622592}\n{'sec': 10.039940357208252, 'minatar-space_invaders/eval_R': 36.97999954223633, 'steps': 630784}\n{'sec': 10.101919174194336, 'minatar-space_invaders/eval_R': 34.029998779296875, 'steps': 638976}\n{'sec': 10.164889812469482, 'minatar-space_invaders/eval_R': 30.1299991607666, 'steps': 647168}\n{'sec': 10.225965023040771, 'minatar-space_invaders/eval_R': 31.31999969482422, 'steps': 655360}\n{'sec': 10.287094593048096, 'minatar-space_invaders/eval_R': 32.11000061035156, 'steps': 663552}\n{'sec': 10.348608255386353, 'minatar-space_invaders/eval_R': 27.26999855041504, 'steps': 671744}\n{'sec': 10.410013437271118, 'minatar-space_invaders/eval_R': 30.989999771118164, 'steps': 679936}\n{'sec': 10.472031116485596, 'minatar-space_invaders/eval_R': 33.380001068115234, 'steps': 688128}\n{'sec': 10.532681703567505, 'minatar-space_invaders/eval_R': 28.09000015258789, 'steps': 696320}\n{'sec': 10.594693660736084, 'minatar-space_invaders/eval_R': 30.729999542236328, 'steps': 704512}\n{'sec': 10.656877994537354, 'minatar-space_invaders/eval_R': 35.05999755859375, 'steps': 712704}\n{'sec': 10.718173265457153, 'minatar-space_invaders/eval_R': 38.65999984741211, 'steps': 720896}\n{'sec': 10.779376029968262, 'minatar-space_invaders/eval_R': 38.07999801635742, 'steps': 729088}\n{'sec': 10.841750621795654, 'minatar-space_invaders/eval_R': 36.959999084472656, 'steps': 737280}\n{'sec': 10.90407919883728, 'minatar-space_invaders/eval_R': 32.82999801635742, 'steps': 745472}\n{'sec': 10.966313600540161, 'minatar-space_invaders/eval_R': 34.349998474121094, 'steps': 753664}\n{'sec': 11.027631521224976, 'minatar-space_invaders/eval_R': 34.81999969482422, 'steps': 761856}\n{'sec': 11.090253591537476, 'minatar-space_invaders/eval_R': 39.040000915527344, 'steps': 770048}\n{'sec': 11.152585506439209, 'minatar-space_invaders/eval_R': 35.39999771118164, 'steps': 778240}\n{'sec': 11.218375205993652, 'minatar-space_invaders/eval_R': 32.97999954223633, 'steps': 786432}\n{'sec': 11.282579183578491, 'minatar-space_invaders/eval_R': 37.0099983215332, 'steps': 794624}\n{'sec': 11.344183444976807, 'minatar-space_invaders/eval_R': 33.18000030517578, 'steps': 802816}\n{'sec': 11.405924558639526, 'minatar-space_invaders/eval_R': 37.72999954223633, 'steps': 811008}\n{'sec': 11.467731475830078, 'minatar-space_invaders/eval_R': 33.290000915527344, 'steps': 819200}\n{'sec': 11.529204845428467, 'minatar-space_invaders/eval_R': 35.05999755859375, 'steps': 827392}\n{'sec': 11.59109616279602, 'minatar-space_invaders/eval_R': 37.16999816894531, 'steps': 835584}\n{'sec': 11.653573989868164, 'minatar-space_invaders/eval_R': 34.43000030517578, 'steps': 843776}\n{'sec': 11.716073751449585, 'minatar-space_invaders/eval_R': 32.84000015258789, 'steps': 851968}\n{'sec': 11.778334617614746, 'minatar-space_invaders/eval_R': 34.0, 'steps': 860160}\n{'sec': 11.842026948928833, 'minatar-space_invaders/eval_R': 35.15999984741211, 'steps': 868352}\n{'sec': 11.903452396392822, 'minatar-space_invaders/eval_R': 35.939998626708984, 'steps': 876544}\n{'sec': 11.96654462814331, 'minatar-space_invaders/eval_R': 37.529998779296875, 'steps': 884736}\n{'sec': 12.029294729232788, 'minatar-space_invaders/eval_R': 34.18000030517578, 'steps': 892928}\n{'sec': 12.091914653778076, 'minatar-space_invaders/eval_R': 39.87999725341797, 'steps': 901120}\n{'sec': 12.153361320495605, 'minatar-space_invaders/eval_R': 37.16999816894531, 'steps': 909312}\n{'sec': 12.216184139251709, 'minatar-space_invaders/eval_R': 40.459999084472656, 'steps': 917504}\n{'sec': 12.2782461643219, 'minatar-space_invaders/eval_R': 38.41999816894531, 'steps': 925696}\n{'sec': 12.340400695800781, 'minatar-space_invaders/eval_R': 38.66999816894531, 'steps': 933888}\n{'sec': 12.401717185974121, 'minatar-space_invaders/eval_R': 38.70000076293945, 'steps': 942080}\n{'sec': 12.464479207992554, 'minatar-space_invaders/eval_R': 41.21999740600586, 'steps': 950272}\n{'sec': 12.526517629623413, 'minatar-space_invaders/eval_R': 36.06999969482422, 'steps': 958464}\n{'sec': 12.588663101196289, 'minatar-space_invaders/eval_R': 35.529998779296875, 'steps': 966656}\n{'sec': 12.650326251983643, 'minatar-space_invaders/eval_R': 35.98999786376953, 'steps': 974848}\n{'sec': 12.71241569519043, 'minatar-space_invaders/eval_R': 37.82999801635742, 'steps': 983040}\n{'sec': 12.774691343307495, 'minatar-space_invaders/eval_R': 42.55999755859375, 'steps': 991232}\n{'sec': 12.836691617965698, 'minatar-space_invaders/eval_R': 33.529998779296875, 'steps': 999424}\n{'sec': 12.899106740951538, 'minatar-space_invaders/eval_R': 38.869998931884766, 'steps': 1007616}\n{'sec': 12.96077823638916, 'minatar-space_invaders/eval_R': 38.66999816894531, 'steps': 1015808}\n{'sec': 13.023319721221924, 'minatar-space_invaders/eval_R': 41.11000061035156, 'steps': 1024000}\n{'sec': 13.085025072097778, 'minatar-space_invaders/eval_R': 39.90999984741211, 'steps': 1032192}\n{'sec': 13.145965099334717, 'minatar-space_invaders/eval_R': 47.54999923706055, 'steps': 1040384}\n{'sec': 13.208592891693115, 'minatar-space_invaders/eval_R': 49.119998931884766, 'steps': 1048576}\n{'sec': 13.271933794021606, 'minatar-space_invaders/eval_R': 41.790000915527344, 'steps': 1056768}\n{'sec': 13.334080457687378, 'minatar-space_invaders/eval_R': 45.869998931884766, 'steps': 1064960}\n{'sec': 13.39561939239502, 'minatar-space_invaders/eval_R': 41.529998779296875, 'steps': 1073152}\n{'sec': 13.458390235900879, 'minatar-space_invaders/eval_R': 45.099998474121094, 'steps': 1081344}\n{'sec': 13.520284652709961, 'minatar-space_invaders/eval_R': 40.209999084472656, 'steps': 1089536}\n{'sec': 13.582741975784302, 'minatar-space_invaders/eval_R': 37.31999969482422, 'steps': 1097728}\n{'sec': 13.645257711410522, 'minatar-space_invaders/eval_R': 43.119998931884766, 'steps': 1105920}\n{'sec': 13.706742763519287, 'minatar-space_invaders/eval_R': 43.869998931884766, 'steps': 1114112}\n{'sec': 13.768507957458496, 'minatar-space_invaders/eval_R': 42.30999755859375, 'steps': 1122304}\n{'sec': 13.83030104637146, 'minatar-space_invaders/eval_R': 41.90999984741211, 'steps': 1130496}\n{'sec': 13.891791343688965, 'minatar-space_invaders/eval_R': 43.84000015258789, 'steps': 1138688}\n{'sec': 13.954205513000488, 'minatar-space_invaders/eval_R': 39.91999816894531, 'steps': 1146880}\n{'sec': 14.016176700592041, 'minatar-space_invaders/eval_R': 46.78999710083008, 'steps': 1155072}\n{'sec': 14.077875852584839, 'minatar-space_invaders/eval_R': 45.09000015258789, 'steps': 1163264}\n{'sec': 14.141515970230103, 'minatar-space_invaders/eval_R': 48.5099983215332, 'steps': 1171456}\n{'sec': 14.202869415283203, 'minatar-space_invaders/eval_R': 48.45000076293945, 'steps': 1179648}\n{'sec': 14.263948202133179, 'minatar-space_invaders/eval_R': 46.68000030517578, 'steps': 1187840}\n{'sec': 14.325766801834106, 'minatar-space_invaders/eval_R': 49.43000030517578, 'steps': 1196032}\n{'sec': 14.389412641525269, 'minatar-space_invaders/eval_R': 46.79999923706055, 'steps': 1204224}\n{'sec': 14.452847719192505, 'minatar-space_invaders/eval_R': 48.54999923706055, 'steps': 1212416}\n{'sec': 14.520816564559937, 'minatar-space_invaders/eval_R': 45.779998779296875, 'steps': 1220608}\n{'sec': 14.58729362487793, 'minatar-space_invaders/eval_R': 46.65999984741211, 'steps': 1228800}\n{'sec': 14.649663925170898, 'minatar-space_invaders/eval_R': 52.53999710083008, 'steps': 1236992}\n{'sec': 14.71155834197998, 'minatar-space_invaders/eval_R': 52.68000030517578, 'steps': 1245184}\n{'sec': 14.77338194847107, 'minatar-space_invaders/eval_R': 57.44999694824219, 'steps': 1253376}\n{'sec': 14.835031032562256, 'minatar-space_invaders/eval_R': 49.56999969482422, 'steps': 1261568}\n{'sec': 14.896517038345337, 'minatar-space_invaders/eval_R': 46.41999816894531, 'steps': 1269760}\n{'sec': 14.958068609237671, 'minatar-space_invaders/eval_R': 43.95000076293945, 'steps': 1277952}\n{'sec': 15.020271301269531, 'minatar-space_invaders/eval_R': 44.40999984741211, 'steps': 1286144}\n{'sec': 15.081310749053955, 'minatar-space_invaders/eval_R': 37.119998931884766, 'steps': 1294336}\n{'sec': 15.143357276916504, 'minatar-space_invaders/eval_R': 41.37999725341797, 'steps': 1302528}\n{'sec': 15.205912113189697, 'minatar-space_invaders/eval_R': 44.13999938964844, 'steps': 1310720}\n{'sec': 15.268203496932983, 'minatar-space_invaders/eval_R': 41.95000076293945, 'steps': 1318912}\n{'sec': 15.329829454421997, 'minatar-space_invaders/eval_R': 45.64999771118164, 'steps': 1327104}\n{'sec': 15.390460014343262, 'minatar-space_invaders/eval_R': 44.54999923706055, 'steps': 1335296}\n{'sec': 15.45268726348877, 'minatar-space_invaders/eval_R': 46.65999984741211, 'steps': 1343488}\n{'sec': 15.514328956604004, 'minatar-space_invaders/eval_R': 47.73999786376953, 'steps': 1351680}\n{'sec': 15.577226638793945, 'minatar-space_invaders/eval_R': 51.209999084472656, 'steps': 1359872}\n{'sec': 15.638475179672241, 'minatar-space_invaders/eval_R': 46.57999801635742, 'steps': 1368064}\n{'sec': 15.70062255859375, 'minatar-space_invaders/eval_R': 47.0099983215332, 'steps': 1376256}\n{'sec': 15.762184143066406, 'minatar-space_invaders/eval_R': 46.7599983215332, 'steps': 1384448}\n{'sec': 15.823283672332764, 'minatar-space_invaders/eval_R': 44.71999740600586, 'steps': 1392640}\n{'sec': 15.885669231414795, 'minatar-space_invaders/eval_R': 47.849998474121094, 'steps': 1400832}\n{'sec': 15.947019577026367, 'minatar-space_invaders/eval_R': 43.95000076293945, 'steps': 1409024}\n{'sec': 16.009851217269897, 'minatar-space_invaders/eval_R': 44.779998779296875, 'steps': 1417216}\n{'sec': 16.072571754455566, 'minatar-space_invaders/eval_R': 45.46999740600586, 'steps': 1425408}\n{'sec': 16.13666296005249, 'minatar-space_invaders/eval_R': 41.36000061035156, 'steps': 1433600}\n{'sec': 16.19773244857788, 'minatar-space_invaders/eval_R': 45.369998931884766, 'steps': 1441792}\n{'sec': 16.25977325439453, 'minatar-space_invaders/eval_R': 40.79999923706055, 'steps': 1449984}\n{'sec': 16.321349620819092, 'minatar-space_invaders/eval_R': 34.21999740600586, 'steps': 1458176}\n{'sec': 16.382468223571777, 'minatar-space_invaders/eval_R': 37.18000030517578, 'steps': 1466368}\n{'sec': 16.444432497024536, 'minatar-space_invaders/eval_R': 36.43000030517578, 'steps': 1474560}\n{'sec': 16.505704164505005, 'minatar-space_invaders/eval_R': 36.439998626708984, 'steps': 1482752}\n{'sec': 16.56830406188965, 'minatar-space_invaders/eval_R': 40.87999725341797, 'steps': 1490944}\n{'sec': 16.62917184829712, 'minatar-space_invaders/eval_R': 35.34000015258789, 'steps': 1499136}\n{'sec': 16.6910183429718, 'minatar-space_invaders/eval_R': 38.47999954223633, 'steps': 1507328}\n{'sec': 16.753369331359863, 'minatar-space_invaders/eval_R': 36.30999755859375, 'steps': 1515520}\n{'sec': 16.816020727157593, 'minatar-space_invaders/eval_R': 39.790000915527344, 'steps': 1523712}\n{'sec': 16.876911163330078, 'minatar-space_invaders/eval_R': 38.790000915527344, 'steps': 1531904}\n{'sec': 16.937928915023804, 'minatar-space_invaders/eval_R': 41.04999923706055, 'steps': 1540096}\n{'sec': 17.00044536590576, 'minatar-space_invaders/eval_R': 46.88999938964844, 'steps': 1548288}\n{'sec': 17.06236720085144, 'minatar-space_invaders/eval_R': 51.84000015258789, 'steps': 1556480}\n{'sec': 17.124619483947754, 'minatar-space_invaders/eval_R': 52.80999755859375, 'steps': 1564672}\n{'sec': 17.190346717834473, 'minatar-space_invaders/eval_R': 52.849998474121094, 'steps': 1572864}\n{'sec': 17.252156496047974, 'minatar-space_invaders/eval_R': 53.029998779296875, 'steps': 1581056}\n{'sec': 17.313416004180908, 'minatar-space_invaders/eval_R': 55.07999801635742, 'steps': 1589248}\n{'sec': 17.374717950820923, 'minatar-space_invaders/eval_R': 53.57999801635742, 'steps': 1597440}\n{'sec': 17.440895795822144, 'minatar-space_invaders/eval_R': 64.98999786376953, 'steps': 1605632}\n{'sec': 17.502805471420288, 'minatar-space_invaders/eval_R': 58.369998931884766, 'steps': 1613824}\n{'sec': 17.565500259399414, 'minatar-space_invaders/eval_R': 56.029998779296875, 'steps': 1622016}\n{'sec': 17.627073287963867, 'minatar-space_invaders/eval_R': 57.54999923706055, 'steps': 1630208}\n{'sec': 17.68840265274048, 'minatar-space_invaders/eval_R': 54.439998626708984, 'steps': 1638400}\n{'sec': 17.750411987304688, 'minatar-space_invaders/eval_R': 51.40999984741211, 'steps': 1646592}\n{'sec': 17.812618017196655, 'minatar-space_invaders/eval_R': 46.21999740600586, 'steps': 1654784}\n{'sec': 17.873721837997437, 'minatar-space_invaders/eval_R': 55.43000030517578, 'steps': 1662976}\n{'sec': 17.934969902038574, 'minatar-space_invaders/eval_R': 51.189998626708984, 'steps': 1671168}\n{'sec': 17.99699854850769, 'minatar-space_invaders/eval_R': 52.53999710083008, 'steps': 1679360}\n{'sec': 18.061479091644287, 'minatar-space_invaders/eval_R': 45.11000061035156, 'steps': 1687552}\n{'sec': 18.123544216156006, 'minatar-space_invaders/eval_R': 51.619998931884766, 'steps': 1695744}\n{'sec': 18.185349702835083, 'minatar-space_invaders/eval_R': 53.81999969482422, 'steps': 1703936}\n{'sec': 18.246805667877197, 'minatar-space_invaders/eval_R': 48.11000061035156, 'steps': 1712128}\n{'sec': 18.309401512145996, 'minatar-space_invaders/eval_R': 56.52000045776367, 'steps': 1720320}\n{'sec': 18.370880603790283, 'minatar-space_invaders/eval_R': 60.32999801635742, 'steps': 1728512}\n{'sec': 18.433701276779175, 'minatar-space_invaders/eval_R': 60.39999771118164, 'steps': 1736704}\n{'sec': 18.496156930923462, 'minatar-space_invaders/eval_R': 67.44999694824219, 'steps': 1744896}\n{'sec': 18.558175563812256, 'minatar-space_invaders/eval_R': 66.0, 'steps': 1753088}\n{'sec': 18.619977235794067, 'minatar-space_invaders/eval_R': 57.529998779296875, 'steps': 1761280}\n{'sec': 18.682039260864258, 'minatar-space_invaders/eval_R': 56.56999969482422, 'steps': 1769472}\n{'sec': 18.743900299072266, 'minatar-space_invaders/eval_R': 51.93000030517578, 'steps': 1777664}\n{'sec': 18.806183338165283, 'minatar-space_invaders/eval_R': 46.029998779296875, 'steps': 1785856}\n{'sec': 18.867675304412842, 'minatar-space_invaders/eval_R': 47.53999710083008, 'steps': 1794048}\n{'sec': 18.929361820220947, 'minatar-space_invaders/eval_R': 56.28999710083008, 'steps': 1802240}\n{'sec': 18.991262197494507, 'minatar-space_invaders/eval_R': 48.36000061035156, 'steps': 1810432}\n{'sec': 19.053192138671875, 'minatar-space_invaders/eval_R': 45.13999938964844, 'steps': 1818624}\n{'sec': 19.1142840385437, 'minatar-space_invaders/eval_R': 40.12999725341797, 'steps': 1826816}\n{'sec': 19.175010681152344, 'minatar-space_invaders/eval_R': 49.21999740600586, 'steps': 1835008}\n{'sec': 19.23693323135376, 'minatar-space_invaders/eval_R': 51.44999694824219, 'steps': 1843200}\n{'sec': 19.297574758529663, 'minatar-space_invaders/eval_R': 61.459999084472656, 'steps': 1851392}\n{'sec': 19.360533475875854, 'minatar-space_invaders/eval_R': 59.93000030517578, 'steps': 1859584}\n{'sec': 19.421623468399048, 'minatar-space_invaders/eval_R': 53.709999084472656, 'steps': 1867776}\n{'sec': 19.483007431030273, 'minatar-space_invaders/eval_R': 67.2699966430664, 'steps': 1875968}\n{'sec': 19.54475450515747, 'minatar-space_invaders/eval_R': 61.59000015258789, 'steps': 1884160}\n{'sec': 19.607359886169434, 'minatar-space_invaders/eval_R': 54.0, 'steps': 1892352}\n{'sec': 19.669052362442017, 'minatar-space_invaders/eval_R': 64.97999572753906, 'steps': 1900544}\n{'sec': 19.731176376342773, 'minatar-space_invaders/eval_R': 69.97999572753906, 'steps': 1908736}\n{'sec': 19.791855812072754, 'minatar-space_invaders/eval_R': 66.25, 'steps': 1916928}\n{'sec': 19.853620290756226, 'minatar-space_invaders/eval_R': 68.4000015258789, 'steps': 1925120}\n{'sec': 19.915037631988525, 'minatar-space_invaders/eval_R': 62.34000015258789, 'steps': 1933312}\n{'sec': 19.982652187347412, 'minatar-space_invaders/eval_R': 54.90999984741211, 'steps': 1941504}\n{'sec': 20.050484895706177, 'minatar-space_invaders/eval_R': 60.459999084472656, 'steps': 1949696}\n{'sec': 20.112106323242188, 'minatar-space_invaders/eval_R': 55.31999969482422, 'steps': 1957888}\n{'sec': 20.17315149307251, 'minatar-space_invaders/eval_R': 62.07999801635742, 'steps': 1966080}\n{'sec': 20.233920574188232, 'minatar-space_invaders/eval_R': 57.459999084472656, 'steps': 1974272}\n{'sec': 20.295720100402832, 'minatar-space_invaders/eval_R': 54.209999084472656, 'steps': 1982464}\n{'sec': 20.358434200286865, 'minatar-space_invaders/eval_R': 58.05999755859375, 'steps': 1990656}\n{'sec': 20.419509887695312, 'minatar-space_invaders/eval_R': 62.5, 'steps': 1998848}\n{'sec': 20.48125672340393, 'minatar-space_invaders/eval_R': 57.439998626708984, 'steps': 2007040}\n{'sec': 20.544013023376465, 'minatar-space_invaders/eval_R': 64.47000122070312, 'steps': 2015232}\n{'sec': 20.605129957199097, 'minatar-space_invaders/eval_R': 60.71999740600586, 'steps': 2023424}\n{'sec': 20.666427850723267, 'minatar-space_invaders/eval_R': 59.28999710083008, 'steps': 2031616}\n{'sec': 20.727126359939575, 'minatar-space_invaders/eval_R': 74.66999816894531, 'steps': 2039808}\n{'sec': 20.788750886917114, 'minatar-space_invaders/eval_R': 65.04000091552734, 'steps': 2048000}\n{'sec': 20.851056575775146, 'minatar-space_invaders/eval_R': 67.5199966430664, 'steps': 2056192}\n{'sec': 20.91232681274414, 'minatar-space_invaders/eval_R': 59.98999786376953, 'steps': 2064384}\n{'sec': 20.97471022605896, 'minatar-space_invaders/eval_R': 60.31999969482422, 'steps': 2072576}\n{'sec': 21.036993265151978, 'minatar-space_invaders/eval_R': 58.22999954223633, 'steps': 2080768}\n{'sec': 21.097667455673218, 'minatar-space_invaders/eval_R': 62.43000030517578, 'steps': 2088960}\n{'sec': 21.160045862197876, 'minatar-space_invaders/eval_R': 74.15999603271484, 'steps': 2097152}\n{'sec': 21.220505237579346, 'minatar-space_invaders/eval_R': 77.75, 'steps': 2105344}\n{'sec': 21.281810760498047, 'minatar-space_invaders/eval_R': 70.01000213623047, 'steps': 2113536}\n","output_type":"stream"}],"execution_count":null}]}