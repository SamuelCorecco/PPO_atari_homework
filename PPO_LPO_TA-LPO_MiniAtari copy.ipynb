{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-07T17:48:38.529646Z",
     "iopub.status.busy": "2024-12-07T17:48:38.529255Z",
     "iopub.status.idle": "2024-12-07T17:50:02.514573Z",
     "shell.execute_reply": "2024-12-07T17:50:02.513748Z",
     "shell.execute_reply.started": "2024-12-07T17:48:38.529595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install minatar\n",
    "!pip install dm-haiku\n",
    "!pip install distrax\n",
    "!pip install pgx\n",
    "!pip install omegaconf\n",
    "!pip install learn2learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:50:02.517574Z",
     "iopub.status.busy": "2024-12-07T17:50:02.516806Z",
     "iopub.status.idle": "2024-12-07T17:50:24.307644Z",
     "shell.execute_reply": "2024-12-07T17:50:24.306931Z",
     "shell.execute_reply.started": "2024-12-07T17:50:02.517533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "from typing import NamedTuple, Literal\n",
    "import distrax\n",
    "import pgx\n",
    "from pgx.experimental import auto_reset\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from omegaconf import OmegaConf\n",
    "from pydantic import BaseModel\n",
    "import wandb\n",
    "import learn2learn as l2l\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T13:11:18.944825Z",
     "iopub.status.busy": "2024-12-06T13:11:18.944542Z",
     "iopub.status.idle": "2024-12-06T13:11:29.076455Z",
     "shell.execute_reply": "2024-12-06T13:11:29.074694Z",
     "shell.execute_reply.started": "2024-12-06T13:11:18.944799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-space_invaders\" \n",
    "    seed: int = 0\n",
    "    lr: float = 1e-3\n",
    "    num_envs: int = 64 \n",
    "    num_eval_envs: int = 100\n",
    "    num_steps: int = 128\n",
    "    total_timesteps: int = int(1e7)\n",
    "    update_epochs: int = 4\n",
    "    minibatch_size: int = 4096\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    wandb_entity: str = \"nonarruginitocalamarodiferro-usi\" \n",
    "    wandb_project: str = \"pgx-minatar-ppo\"  \n",
    "    save_model: bool = False\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "\n",
    "args = PPOConfig(\n",
    "    env_name=\"minatar-space_invaders\", \n",
    "    seed=0,\n",
    "    lr=1e-3,\n",
    "    num_envs=64,\n",
    "    num_eval_envs=100,\n",
    "    num_steps=128,\n",
    "    total_timesteps=int(1e7), \n",
    "    update_epochs=4,\n",
    "    minibatch_size=4096, \n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.90,\n",
    "    clip_eps=0.25,\n",
    "    ent_coef=0.005, \n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5, \n",
    "    wandb_entity=\"nonarruginitocalamarodiferro-usi\", \n",
    "    wandb_project=\"pgx-minatar-ppo\", \n",
    "    save_model=False  \n",
    ")\n",
    "\n",
    "print(args)  # To verify your updated configuration\n",
    "\n",
    "# Initialize environment\n",
    "env = pgx.make(str(args.env_name))\n",
    "\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ActorCritic(hk.Module):\n",
    "    def __init__(self, num_actions, activation=\"tanh\"):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.activation = activation\n",
    "        assert activation in [\"relu\", \"tanh\"]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32)\n",
    "        if self.activation == \"relu\":\n",
    "            activation = jax.nn.relu\n",
    "        else:\n",
    "            activation = jax.nn.tanh\n",
    "        x = hk.Conv2D(32, kernel_shape=2)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = hk.avg_pool(x, window_shape=(2, 2),\n",
    "                        strides=(2, 2), padding=\"VALID\")\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = hk.Linear(64)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        actor_mean = hk.Linear(64)(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = hk.Linear(64)(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = hk.Linear(self.num_actions)(actor_mean)\n",
    "\n",
    "        critic = hk.Linear(64)(x)\n",
    "        critic = activation(critic)\n",
    "        critic = hk.Linear(64)(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = hk.Linear(1)(critic)\n",
    "\n",
    "        return actor_mean, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "def forward_fn(x, is_eval=False):\n",
    "    net = ActorCritic(env.num_actions, activation=\"tanh\")\n",
    "    logits, value = net(x)\n",
    "    return logits, value\n",
    "\n",
    "\n",
    "forward = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "\n",
    "\n",
    "optimizer = optax.chain(optax.clip_by_global_norm(\n",
    "    args.max_grad_norm), optax.adam(args.lr, eps=1e-5))\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_update_fn():\n",
    "    # TRAIN LOOP\n",
    "    def _update_step(runner_state):\n",
    "        # COLLECT TRAJECTORIES\n",
    "        step_fn = jax.vmap(auto_reset(env.step, env.init))\n",
    "\n",
    "        def _env_step(runner_state, unused):\n",
    "            params, opt_state, env_state, last_obs, rng = runner_state\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = forward.apply(params, last_obs)\n",
    "            pi = distrax.Categorical(logits=logits)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            keys = jax.random.split(_rng, env_state.observation.shape[0])\n",
    "            env_state = step_fn(env_state, action, keys)\n",
    "            transition = Transition(\n",
    "                env_state.terminated,\n",
    "                action,\n",
    "                value,\n",
    "                jnp.squeeze(env_state.rewards),\n",
    "                log_prob,\n",
    "                last_obs\n",
    "            )\n",
    "            runner_state = (params, opt_state, env_state,\n",
    "                            env_state.observation, rng)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, runner_state, None, args.num_steps\n",
    "        )\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "        params, opt_state, env_state, last_obs, rng = runner_state\n",
    "        _, last_val = forward.apply(params, last_obs)\n",
    "\n",
    "        def _calculate_gae(traj_batch, last_val):\n",
    "            def _get_advantages(gae_and_next_value, transition):\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                )\n",
    "                delta = reward + args.gamma * next_value * (1 - done) - value\n",
    "                gae = (\n",
    "                    delta\n",
    "                    + args.gamma * args.gae_lambda * (1 - done) * gae\n",
    "                )\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return advantages, advantages + traj_batch.value\n",
    "\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        # UPDATE NETWORK\n",
    "        def _update_epoch(update_state, unused):\n",
    "            def _update_minbatch(tup, batch_info):\n",
    "                params, opt_state = tup\n",
    "                traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                def _loss_fn(params, traj_batch, gae, targets):\n",
    "                    # RERUN NETWORK\n",
    "                    logits, value = forward.apply(params, traj_batch.obs)\n",
    "                    pi = distrax.Categorical(logits=logits)\n",
    "                    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                    # CALCULATE VALUE LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (\n",
    "                        value - traj_batch.value\n",
    "                    ).clip(-args.clip_eps, args.clip_eps)\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(\n",
    "                        value_pred_clipped - targets)\n",
    "                    value_loss = (\n",
    "                        0.5 * jnp.maximum(value_losses,\n",
    "                                          value_losses_clipped).mean()\n",
    "                    )\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = (\n",
    "                        jnp.clip(\n",
    "                            ratio,\n",
    "                            1.0 - args.clip_eps,\n",
    "                            1.0 + args.clip_eps,\n",
    "                        )\n",
    "                        * gae\n",
    "                    )\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                    loss_actor = loss_actor.mean()\n",
    "                    entropy = pi.entropy().mean()\n",
    "\n",
    "                    total_loss = (\n",
    "                        loss_actor\n",
    "                        + args.vf_coef * value_loss\n",
    "                        - args.ent_coef * entropy\n",
    "                    )\n",
    "                    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                total_loss, grads = grad_fn(\n",
    "                    params, traj_batch, advantages, targets)\n",
    "                updates, opt_state = optimizer.update(grads, opt_state)\n",
    "                params = optax.apply_updates(params, updates)\n",
    "                return (params, opt_state), total_loss\n",
    "\n",
    "            params, opt_state, traj_batch, advantages, targets, rng = update_state\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            batch_size = args.minibatch_size * num_minibatches\n",
    "            assert (\n",
    "                batch_size == args.num_steps * args.num_envs\n",
    "            ), \"batch size must be equal to number of steps * number of envs\"\n",
    "            permutation = jax.random.permutation(_rng, batch_size)\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree_util.tree_map(\n",
    "                lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "            )\n",
    "            shuffled_batch = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "            )\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(\n",
    "                    x, [num_minibatches, -1] + list(x.shape[1:])\n",
    "                ),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (params, opt_state),  total_loss = jax.lax.scan(\n",
    "                _update_minbatch, (params, opt_state), minibatches\n",
    "            )\n",
    "            update_state = (params, opt_state, traj_batch,\n",
    "                            advantages, targets, rng)\n",
    "            return update_state, total_loss\n",
    "\n",
    "        update_state = (params, opt_state, traj_batch,\n",
    "                        advantages, targets, rng)\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, args.update_epochs\n",
    "        )\n",
    "        params, opt_state, _, _, _, rng = update_state\n",
    "\n",
    "        runner_state = (params, opt_state, env_state, last_obs, rng)\n",
    "        return runner_state, loss_info\n",
    "    return _update_step\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, rng_key):\n",
    "    step_fn = jax.vmap(env.step)\n",
    "    rng_key, sub_key = jax.random.split(rng_key)\n",
    "    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n",
    "    state = jax.vmap(env.init)(subkeys)\n",
    "    R = jnp.zeros_like(state.rewards)\n",
    "\n",
    "    def cond_fn(tup):\n",
    "        state, _, _ = tup\n",
    "        return ~state.terminated.all()\n",
    "\n",
    "    def loop_fn(tup):\n",
    "        state, R, rng_key = tup\n",
    "        logits, value = forward.apply(params, state.observation)\n",
    "        # action = logits.argmax(axis=-1)\n",
    "        pi = distrax.Categorical(logits=logits)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        action = pi.sample(seed=_rng)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        keys = jax.random.split(_rng, state.observation.shape[0])\n",
    "        state = step_fn(state, action, keys)\n",
    "        return state, R + state.rewards, rng_key\n",
    "    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n",
    "    return R.mean()\n",
    "\n",
    "\n",
    "def train(rng):\n",
    "    tt = 0\n",
    "    st = time.time()\n",
    "    # INIT NETWORK\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    init_x = jnp.zeros((1, ) + env.observation_shape)\n",
    "    params = forward.init(_rng, init_x)\n",
    "    opt_state = optimizer.init(params=params)\n",
    "\n",
    "    # INIT UPDATE FUNCTION\n",
    "    _update_step = make_update_fn()\n",
    "    jitted_update_step = jax.jit(_update_step)\n",
    "\n",
    "    # INIT ENV\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, args.num_envs)\n",
    "    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (params, opt_state, env_state, env_state.observation, _rng)\n",
    "\n",
    "    # warm up\n",
    "    _, _ = jitted_update_step(runner_state)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    # initial evaluation\n",
    "    et = time.time()  # exclude evaluation time\n",
    "    tt += et - st\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    eval_R = evaluate(runner_state[0], _rng)\n",
    "    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "    print(log)\n",
    "    wandb.log(log)\n",
    "    st = time.time()\n",
    "\n",
    "    for i in range(num_updates):\n",
    "        runner_state, loss_info = jitted_update_step(runner_state)\n",
    "        steps += args.num_envs * args.num_steps\n",
    "\n",
    "        # evaluation\n",
    "        et = time.time()  # exclude evaluation time\n",
    "        tt += et - st\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        eval_R = evaluate(runner_state[0], _rng)\n",
    "        log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "        print(log)\n",
    "        wandb.log(log)\n",
    "        st = time.time()\n",
    "\n",
    "    return runner_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args.dict())\n",
    "    rng = jax.random.PRNGKey(args.seed)\n",
    "    out = train(rng)\n",
    "    if args.save_model:\n",
    "        with open(f\"{args.env_name}-seed={args.seed}.ckpt\", \"wb\") as f:\n",
    "            pickle.dump(out[0], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:11:52.473337Z",
     "iopub.status.busy": "2024-12-07T18:11:52.473018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-space_invaders\" \n",
    "    seed: int = 2\n",
    "    lr: float = 5e-4  \n",
    "    num_envs: int = 2048 \n",
    "    num_eval_envs: int = 128  \n",
    "    num_steps: int = 5  \n",
    "    total_timesteps: int = 10000000  \n",
    "    update_epochs: int = 4 \n",
    "    minibatch_size: int = 1024 \n",
    "    num_minibatches: int = 32  \n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.25\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 8.0\n",
    "    dpo_alpha: float = 8.0\n",
    "    dpo_beta: float = 0.6\n",
    "    wandb_entity: str = \"nonarruginitocalamarodiferro-usi\"\n",
    "    wandb_project: str = \"pgx-minatar-ppo\"\n",
    "    save_model: bool = False\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "args = DPOConfig()\n",
    "\n",
    "env = pgx.make(str(args.env_name))\n",
    "\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n",
    "\n",
    "# Modifica della rete ActorCritic per 1 hidden layer con 128 unità\n",
    "class ActorCritic(hk.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32)\n",
    "        # Un singolo hidden layer con 128 unità\n",
    "        x = x.reshape((x.shape[0], -1))  # Flatten\n",
    "        x = hk.Linear(128)(x)  # Hidden layer con 128 unità\n",
    "        x = jax.nn.relu(x)  # Attivazione relu\n",
    "\n",
    "        actor_mean = hk.Linear(self.num_actions)(x)\n",
    "\n",
    "        critic = hk.Linear(1)(x)\n",
    "\n",
    "        return actor_mean, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "def forward_fn(x, is_eval=False):\n",
    "    net = ActorCritic(env.num_actions)\n",
    "    logits, value = net(x)\n",
    "    return logits, value\n",
    "\n",
    "forward = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(args.max_grad_norm),\n",
    "    optax.adam(args.lr, eps=1e-5)\n",
    ")\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "\n",
    "def make_update_fn():\n",
    "    # TRAIN LOOP\n",
    "    def _update_step(runner_state):\n",
    "        # COLLECT TRAJECTORIES\n",
    "        step_fn = jax.vmap(auto_reset(env.step, env.init))\n",
    "\n",
    "        def _env_step(runner_state, unused):\n",
    "            params, opt_state, env_state, last_obs, rng = runner_state\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = forward.apply(params, last_obs)\n",
    "            pi = distrax.Categorical(logits=logits)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            keys = jax.random.split(_rng, env_state.observation.shape[0])\n",
    "            env_state = step_fn(env_state, action, keys)\n",
    "            transition = Transition(\n",
    "                env_state.terminated,\n",
    "                action,\n",
    "                value,\n",
    "                jnp.squeeze(env_state.rewards),\n",
    "                log_prob,\n",
    "                last_obs\n",
    "            )\n",
    "            runner_state = (params, opt_state, env_state,\n",
    "                            env_state.observation, rng)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, runner_state, None, args.num_steps\n",
    "        )\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "        params, opt_state, env_state, last_obs, rng = runner_state\n",
    "        _, last_val = forward.apply(params, last_obs)\n",
    "\n",
    "        def _calculate_gae(traj_batch, last_val):\n",
    "            def _get_advantages(gae_and_next_value, transition):\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                )\n",
    "                delta = reward + args.gamma * next_value * (1 - done) - value\n",
    "                gae = (\n",
    "                    delta\n",
    "                    + args.gamma * args.gae_lambda * (1 - done) * gae\n",
    "                )\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return advantages, advantages + traj_batch.value\n",
    "\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        # UPDATE NETWORK\n",
    "        def _update_epoch(update_state, unused):\n",
    "            def _update_minbatch(tup, batch_info):\n",
    "                params, opt_state = tup\n",
    "                traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                def _loss_fn(params, traj_batch, gae, targets):\n",
    "                    # RERUN NETWORK\n",
    "                    logits, value = forward.apply(params, traj_batch.obs)\n",
    "                    pi = distrax.Categorical(logits=logits)\n",
    "                    log_prob = pi.log_prob(traj_batch.action)\n",
    "                \n",
    "                    # CRITIC LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (\n",
    "                        value - traj_batch.value\n",
    "                    ).clip(-args.clip_eps, args.clip_eps)\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                \n",
    "                    # DPO\n",
    "\n",
    "                    alpha = args.dpo_alpha\n",
    "                    beta = args.dpo_beta \n",
    "                    log_diff = log_prob - traj_batch.log_prob\n",
    "                    ratio = jnp.exp(log_diff)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    is_pos = (gae >= 0.0).astype(\"float32\")\n",
    "                    r1 = ratio - 1.0\n",
    "                    drift1 = nn.relu(r1 * gae - alpha * nn.tanh(r1 * gae / alpha))\n",
    "                    drift2 = nn.relu(\n",
    "                        log_diff * gae - beta * nn.tanh(log_diff * gae / beta)\n",
    "                    )\n",
    "                    drift = drift1 * is_pos + drift2 * (1 - is_pos)\n",
    "                    loss_actor = -(ratio * gae - drift).mean()\n",
    "\n",
    "                    entropy = pi.entropy().mean()\n",
    "                \n",
    "                    # TOTAL LOSS\n",
    "                    total_loss = (\n",
    "                        loss_actor\n",
    "                        + args.vf_coef * value_loss\n",
    "                        - args.ent_coef * entropy\n",
    "                    )\n",
    "                    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                total_loss, grads = grad_fn(\n",
    "                    params, traj_batch, advantages, targets)\n",
    "                updates, opt_state = optimizer.update(grads, opt_state)\n",
    "                params = optax.apply_updates(params, updates)\n",
    "                return (params, opt_state), total_loss\n",
    "\n",
    "            params, opt_state, traj_batch, advantages, targets, rng = update_state\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            batch_size = args.minibatch_size * num_minibatches\n",
    "            assert (\n",
    "                batch_size == args.num_steps * args.num_envs\n",
    "            ), \"batch size must be equal to number of steps * number of envs\"\n",
    "            permutation = jax.random.permutation(_rng, batch_size)\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree_util.tree_map(\n",
    "                lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "            )\n",
    "            shuffled_batch = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "            )\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(\n",
    "                    x, [num_minibatches, -1] + list(x.shape[1:])\n",
    "                ),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (params, opt_state),  total_loss = jax.lax.scan(\n",
    "                _update_minbatch, (params, opt_state), minibatches\n",
    "            )\n",
    "            update_state = (params, opt_state, traj_batch,\n",
    "                            advantages, targets, rng)\n",
    "            return update_state, total_loss\n",
    "\n",
    "        update_state = (params, opt_state, traj_batch,\n",
    "                        advantages, targets, rng)\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, args.update_epochs\n",
    "        )\n",
    "        params, opt_state, _, _, _, rng = update_state\n",
    "\n",
    "        runner_state = (params, opt_state, env_state, last_obs, rng)\n",
    "        return runner_state, loss_info\n",
    "    return _update_step\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, rng_key):\n",
    "    step_fn = jax.vmap(env.step)\n",
    "    rng_key, sub_key = jax.random.split(rng_key)\n",
    "    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n",
    "    state = jax.vmap(env.init)(subkeys)\n",
    "    R = jnp.zeros_like(state.rewards)\n",
    "\n",
    "    def cond_fn(tup):\n",
    "        state, _, _ = tup\n",
    "        return ~state.terminated.all()\n",
    "\n",
    "    def loop_fn(tup):\n",
    "        state, R, rng_key = tup\n",
    "        logits, value = forward.apply(params, state.observation)\n",
    "        # action = logits.argmax(axis=-1)\n",
    "        pi = distrax.Categorical(logits=logits)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        action = pi.sample(seed=_rng)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        keys = jax.random.split(_rng, state.observation.shape[0])\n",
    "        state = step_fn(state, action, keys)\n",
    "        return state, R + state.rewards, rng_key\n",
    "    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n",
    "    return R.mean()\n",
    "\n",
    "\n",
    "def train(rng):\n",
    "    tt = 0\n",
    "    st = time.time()\n",
    "    # INIT NETWORK\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    init_x = jnp.zeros((1, ) + env.observation_shape)\n",
    "    params = forward.init(_rng, init_x)\n",
    "    opt_state = optimizer.init(params=params)\n",
    "\n",
    "    # INIT UPDATE FUNCTION\n",
    "    _update_step = make_update_fn()\n",
    "    jitted_update_step = jax.jit(_update_step)\n",
    "\n",
    "    # INIT ENV\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, args.num_envs)\n",
    "    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (params, opt_state, env_state, env_state.observation, _rng)\n",
    "\n",
    "    # warm up\n",
    "    _, _ = jitted_update_step(runner_state)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    # initial evaluation\n",
    "    et = time.time()  # exclude evaluation time\n",
    "    tt += et - st\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    eval_R = evaluate(runner_state[0], _rng)\n",
    "    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "    print(log)\n",
    "    wandb.log(log)\n",
    "    st = time.time()\n",
    "\n",
    "    for i in range(num_updates):\n",
    "        runner_state, loss_info = jitted_update_step(runner_state)\n",
    "        steps += args.num_envs * args.num_steps\n",
    "\n",
    "        # evaluation\n",
    "        et = time.time()  # exclude evaluation time\n",
    "        tt += et - st\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        eval_R = evaluate(runner_state[0], _rng)\n",
    "        log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "        print(log)\n",
    "        wandb.log(log)\n",
    "        st = time.time()\n",
    "\n",
    "    return runner_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args.dict())\n",
    "    rng = jax.random.PRNGKey(args.seed)\n",
    "    out = train(rng)\n",
    "    if args.save_model:\n",
    "        with open(f\"{args.env_name}-seed={args.seed}.ckpt\", \"wb\") as f:\n",
    "            pickle.dump(out[0], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-space_invaders\"\n",
    "    seed: int = 2\n",
    "    lr: float = 5e-4\n",
    "    num_envs: int = 2048\n",
    "    num_eval_envs: int = 128\n",
    "    num_steps: int = 5\n",
    "    total_timesteps: int = 10000000\n",
    "    update_epochs: int = 4\n",
    "    minibatch_size: int = 1024\n",
    "    num_minibatches: int = 32\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.25\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 8.0\n",
    "    wandb_entity: str = \"nonarruginitocalamarodiferro-usi\"\n",
    "    wandb_project: str = \"pgx-minatar-lpo\"\n",
    "    save_model: bool = False\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "\n",
    "args = LPOConfig()\n",
    "\n",
    "env = pgx.make(str(args.env_name))\n",
    "\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n",
    "\n",
    "\n",
    "class DriftFunction(hk.Module):\n",
    "    \"\"\"Learned drift function for LPO.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, log_prob_ratio, advantage):\n",
    "        # Input transformations for stability\n",
    "        log_diff = log_prob_ratio\n",
    "        inputs = jnp.stack([log_diff, advantage, log_diff * advantage], axis=-1)\n",
    "\n",
    "        # A single hidden layer with 128 units and ReLU activation\n",
    "        hidden = hk.Linear(128)(inputs)\n",
    "        hidden = jax.nn.relu(hidden)\n",
    "\n",
    "        # Output a scalar for the drift value\n",
    "        drift = hk.Linear(1)(hidden)\n",
    "        return jnp.squeeze(drift, axis=-1)\n",
    "\n",
    "\n",
    "class ActorCritic(hk.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32)\n",
    "        x = x.reshape((x.shape[0], -1))  # Flatten\n",
    "        x = hk.Linear(128)(x)  # Hidden layer with 128 units\n",
    "        x = jax.nn.relu(x)\n",
    "\n",
    "        actor_mean = hk.Linear(self.num_actions)(x)\n",
    "        critic = hk.Linear(1)(x)\n",
    "\n",
    "        return actor_mean, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "def forward_fn(x):\n",
    "    net = ActorCritic(env.num_actions)\n",
    "    logits, value = net(x)\n",
    "    return logits, value\n",
    "\n",
    "\n",
    "def drift_fn(log_prob_ratio, advantage):\n",
    "    drift_net = DriftFunction()\n",
    "    return drift_net(log_prob_ratio, advantage)\n",
    "\n",
    "\n",
    "forward = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "drift = hk.without_apply_rng(hk.transform(drift_fn))\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(args.max_grad_norm),\n",
    "    optax.adam(args.lr, eps=1e-5)\n",
    ")\n",
    "\n",
    "drift_optimizer = optax.adam(args.lr, eps=1e-5)\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_update_fn():\n",
    "    def _update_step(runner_state):\n",
    "        # Trajectory collection (same as in DPO)\n",
    "        step_fn = jax.vmap(auto_reset(env.step, env.init))\n",
    "\n",
    "        def _env_step(runner_state, unused):\n",
    "            params, opt_state, env_state, last_obs, rng, drift_params = runner_state\n",
    "\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = forward.apply(params, last_obs)\n",
    "            pi = distrax.Categorical(logits=logits)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            keys = jax.random.split(_rng, env_state.observation.shape[0])\n",
    "            env_state = step_fn(env_state, action, keys)\n",
    "            transition = Transition(\n",
    "                env_state.terminated,\n",
    "                action,\n",
    "                value,\n",
    "                jnp.squeeze(env_state.rewards),\n",
    "                log_prob,\n",
    "                last_obs\n",
    "            )\n",
    "            runner_state = (params, opt_state, env_state, env_state.observation, rng, drift_params)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, runner_state, None, args.num_steps\n",
    "        )\n",
    "\n",
    "        # Calculate Advantage\n",
    "        params, opt_state, env_state, last_obs, rng, drift_params = runner_state\n",
    "        _, last_val = forward.apply(params, last_obs)\n",
    "\n",
    "        def _calculate_gae(traj_batch, last_val):\n",
    "            def _get_advantages(gae_and_next_value, transition):\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward = transition.done, transition.value, transition.reward\n",
    "                delta = reward + args.gamma * next_value * (1 - done) - value\n",
    "                gae = delta + args.gamma * args.gae_lambda * (1 - done) * gae\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return advantages, advantages + traj_batch.value\n",
    "\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        # Update Network\n",
    "        def _update_minibatch(tup, batch_info):\n",
    "            params, opt_state, drift_params = tup\n",
    "            traj_batch, advantages, targets = batch_info\n",
    "\n",
    "            def _loss_fn(params, drift_params, traj_batch, advantages, targets):\n",
    "                # Rerun network\n",
    "                logits, value = forward.apply(params, traj_batch.obs)\n",
    "                pi = distrax.Categorical(logits=logits)\n",
    "                log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                # Calculate actor loss\n",
    "                log_prob_ratio = log_prob - traj_batch.log_prob\n",
    "                drift_values = drift.apply(drift_params, log_prob_ratio, advantages)\n",
    "                loss_actor = -(jnp.exp(log_prob_ratio) * advantages - drift_values).mean()\n",
    "\n",
    "                # Critic loss\n",
    "                value_pred_clipped = traj_batch.value + (\n",
    "                    value - traj_batch.value\n",
    "                ).clip(-args.clip_eps, args.clip_eps)\n",
    "                value_loss = 0.5 * jnp.square(value - targets).mean()\n",
    "\n",
    "                # Entropy bonus\n",
    "                entropy = pi.entropy().mean()\n",
    "\n",
    "                total_loss = loss_actor + args.vf_coef * value_loss - args.ent_coef * entropy\n",
    "                return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "            grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "            total_loss, grads = grad_fn(params, drift_params, traj_batch, advantages, targets)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "\n",
    "            return (params, opt_state, drift_params), total_loss\n",
    "\n",
    "        update_state = (params, opt_state, drift_params, traj_batch, advantages, targets)\n",
    "        update_state, _ = jax.lax.scan(\n",
    "            _update_minibatch, update_state, None, args.update_epochs\n",
    "        )\n",
    "        params, opt_state, drift_params, _, _, _ = update_state\n",
    "\n",
    "        runner_state = (params, opt_state, env_state, last_obs, rng, drift_params)\n",
    "        return runner_state, traj_batch\n",
    "\n",
    "    return _update_step\n",
    "\n",
    "\n",
    "# Training loop can remain mostly unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TA-LPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TA_LPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-space_invaders\"\n",
    "    seed: int = 2\n",
    "    lr: float = 5e-4\n",
    "    num_envs: int = 2048\n",
    "    num_eval_envs: int = 128\n",
    "    num_steps: int = 5\n",
    "    total_timesteps: int = 10000000\n",
    "    update_epochs: int = 4\n",
    "    minibatch_size: int = 1024\n",
    "    num_minibatches: int = 32\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.25\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 8.0\n",
    "    wandb_entity: str = \"nonarruginitocalamarodiferro-usi\"\n",
    "    wandb_project: str = \"pgx-minatar-ta-lpo\"\n",
    "    save_model: bool = False\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "\n",
    "args = TA_LPOConfig()\n",
    "\n",
    "env = pgx.make(str(args.env_name))\n",
    "\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size\n",
    "\n",
    "\n",
    "class DriftFunction(hk.Module):\n",
    "    \"\"\"Learned drift function for TA-LPO.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, log_prob_ratio, advantage, relative_lifetime):\n",
    "        # Input transformations for stability\n",
    "        inputs = jnp.stack([log_prob_ratio, advantage, relative_lifetime], axis=-1)\n",
    "\n",
    "        # A single hidden layer with 128 units and ReLU activation\n",
    "        hidden = hk.Linear(128)(inputs)\n",
    "        hidden = jax.nn.relu(hidden)\n",
    "\n",
    "        # Output a scalar for the drift value\n",
    "        drift = hk.Linear(1)(hidden)\n",
    "        return jnp.squeeze(drift, axis=-1)\n",
    "\n",
    "\n",
    "class ActorCritic(hk.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def __call__(self, x, relative_lifetime):\n",
    "        x = x.astype(jnp.float32)\n",
    "        x = x.reshape((x.shape[0], -1))  # Flatten\n",
    "        x = hk.Linear(128)(x)  # Hidden layer with 128 units\n",
    "        x = jax.nn.relu(x)\n",
    "\n",
    "        # Concatenate relative lifetime\n",
    "        x = jnp.concatenate([x, jnp.expand_dims(relative_lifetime, axis=-1)], axis=-1)\n",
    "\n",
    "        actor_mean = hk.Linear(self.num_actions)(x)\n",
    "        critic = hk.Linear(1)(x)\n",
    "\n",
    "        return actor_mean, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "def forward_fn(x, relative_lifetime):\n",
    "    net = ActorCritic(env.num_actions)\n",
    "    logits, value = net(x, relative_lifetime)\n",
    "    return logits, value\n",
    "\n",
    "\n",
    "def drift_fn(log_prob_ratio, advantage, relative_lifetime):\n",
    "    drift_net = DriftFunction()\n",
    "    return drift_net(log_prob_ratio, advantage, relative_lifetime)\n",
    "\n",
    "\n",
    "forward = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "drift = hk.without_apply_rng(hk.transform(drift_fn))\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(args.max_grad_norm),\n",
    "    optax.adam(args.lr, eps=1e-5)\n",
    ")\n",
    "\n",
    "drift_optimizer = optax.adam(args.lr, eps=1e-5)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    relative_lifetime: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_update_fn():\n",
    "    def _update_step(runner_state):\n",
    "        # Trajectory collection\n",
    "        step_fn = jax.vmap(auto_reset(env.step, env.init))\n",
    "\n",
    "        def _env_step(runner_state, unused):\n",
    "            params, opt_state, env_state, last_obs, rng, drift_params, step_count = runner_state\n",
    "            relative_lifetime = step_count / args.total_timesteps\n",
    "\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = forward.apply(params, last_obs, relative_lifetime)\n",
    "            pi = distrax.Categorical(logits=logits)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            keys = jax.random.split(_rng, env_state.observation.shape[0])\n",
    "            env_state = step_fn(env_state, action, keys)\n",
    "            transition = Transition(\n",
    "                env_state.terminated,\n",
    "                action,\n",
    "                value,\n",
    "                jnp.squeeze(env_state.rewards),\n",
    "                log_prob,\n",
    "                last_obs,\n",
    "                jnp.full((env_state.observation.shape[0],), relative_lifetime)\n",
    "            )\n",
    "            runner_state = (params, opt_state, env_state, env_state.observation, rng, drift_params, step_count + args.num_steps)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, runner_state, None, args.num_steps\n",
    "        )\n",
    "\n",
    "        # Calculate Advantage\n",
    "        params, opt_state, env_state, last_obs, rng, drift_params, step_count = runner_state\n",
    "        _, last_val = forward.apply(params, last_obs, step_count / args.total_timesteps)\n",
    "\n",
    "        def _calculate_gae(traj_batch, last_val):\n",
    "            def _get_advantages(gae_and_next_value, transition):\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward = transition.done, transition.value, transition.reward\n",
    "                delta = reward + args.gamma * next_value * (1 - done) - value\n",
    "                gae = delta + args.gamma * args.gae_lambda * (1 - done) * gae\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return advantages, advantages + traj_batch.value\n",
    "\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        # Update Network\n",
    "        def _update_minibatch(tup, batch_info):\n",
    "            params, opt_state, drift_params = tup\n",
    "            traj_batch, advantages, targets = batch_info\n",
    "\n",
    "            def _loss_fn(params, drift_params, traj_batch, advantages, targets):\n",
    "                # Rerun network\n",
    "                logits, value = forward.apply(params, traj_batch.obs, traj_batch.relative_lifetime)\n",
    "                pi = distrax.Categorical(logits=logits)\n",
    "                log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                # Calculate actor loss\n",
    "                log_prob_ratio = log_prob - traj_batch.log_prob\n",
    "                drift_values = drift.apply(drift_params, log_prob_ratio, advantages, traj_batch.relative_lifetime)\n",
    "                loss_actor = -(jnp.exp(log_prob_ratio) * advantages - drift_values).mean()\n",
    "\n",
    "                # Critic loss\n",
    "                value_pred_clipped = traj_batch.value + (\n",
    "                    value - traj_batch.value\n",
    "                ).clip(-args.clip_eps, args.clip_eps)\n",
    "                value_loss = 0.5 * jnp.square(value - targets).mean()\n",
    "\n",
    "                # Entropy bonus\n",
    "                entropy = pi.entropy().mean()\n",
    "\n",
    "                total_loss = loss_actor + args.vf_coef * value_loss - args.ent_coef * entropy\n",
    "                return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "            grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "            total_loss, grads = grad_fn(params, drift_params, traj_batch, advantages, targets)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "\n",
    "            return (params, opt_state, drift_params), total_loss\n",
    "\n",
    "        update_state = (params, opt_state, drift_params, traj_batch, advantages, targets)\n",
    "        update_state, _ = jax.lax.scan(\n",
    "            _update_minibatch, update_state, None, args.update_epochs\n",
    "        )\n",
    "        params, opt_state, drift_params, _, _, _ = update_state\n",
    "\n",
    "        runner_state = (params, opt_state, env_state, last_obs, rng, drift_params, step_count)\n",
    "        return runner_state, traj_batch\n",
    "\n",
    "    return _update_step\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, rng_key):\n",
    "    step_fn = jax.vmap(env.step)\n",
    "    rng_key, sub_key = jax.random.split(rng_key)\n",
    "    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n",
    "    state = jax.vmap(env.init)(subkeys)\n",
    "    R = jnp.zeros_like(state.rewards)\n",
    "\n",
    "    def cond_fn(tup):\n",
    "        state, _, _ = tup\n",
    "        return ~state.terminated.all()\n",
    "\n",
    "    def loop_fn(tup):\n",
    "        state, R, rng_key = tup\n",
    "        logits, value = forward.apply(params, state.observation, 0)  # Default relative lifetime for eval\n",
    "        pi = distrax.Categorical(logits=logits)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        action = pi.sample(seed=_rng)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        keys = jax.random.split(_rng, state.observation.shape[0])\n",
    "        state = step_fn(state, action, keys)\n",
    "        return state, R + state.rewards, rng_key\n",
    "\n",
    "    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n",
    "    return R.mean()\n",
    "\n",
    "\n",
    "def train(rng):\n",
    "    tt = 0\n",
    "    st = time.time()\n",
    "\n",
    "    # Initialize network parameters\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    init_x = jnp.zeros((1, ) + env.observation_shape)\n",
    "    params = forward.init(_rng, init_x, 0)\n",
    "    drift_params = drift.init(_rng, jnp.zeros((1,)), jnp.zeros((1,)), 0)  # Initialize drift parameters\n",
    "    opt_state = optimizer.init(params=params)\n",
    "    drift_opt_state = drift_optimizer.init(drift_params)\n",
    "\n",
    "    # Initialize update function\n",
    "    _update_step = make_update_fn()\n",
    "    jitted_update_step = jax.jit(_update_step)\n",
    "\n",
    "    # Initialize environment\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, args.num_envs)\n",
    "    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (params, opt_state, env_state, env_state.observation, _rng, drift_params, 0)\n",
    "\n",
    "    # Warm-up step\n",
    "    _, _ = jitted_update_step(runner_state)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    # Initial evaluation\n",
    "    et = time.time()  # Exclude evaluation time\n",
    "    tt += et - st\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    eval_R = evaluate(runner_state[0], _rng)\n",
    "    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "    print(log)\n",
    "    wandb.log(log)\n",
    "    st = time.time()\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(num_updates):\n",
    "        runner_state, loss_info = jitted_update_step(runner_state)\n",
    "        steps += args.num_envs * args.num_steps\n",
    "\n",
    "        # Evaluation\n",
    "        et = time.time()  # Exclude evaluation time\n",
    "        tt += et - st\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        eval_R = evaluate(runner_state[0], _rng)\n",
    "        log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "        print(log)\n",
    "        wandb.log(log)\n",
    "        st = time.time()\n",
    "\n",
    "    return runner_state\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args.dict())\n",
    "    rng = jax.random.PRNGKey(args.seed)\n",
    "    out = train(rng)\n",
    "    if args.save_model:\n",
    "        with open(f\"{args.env_name}-seed={args.seed}.ckpt\", \"wb\") as f:\n",
    "            pickle.dump(out[0], f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
